{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras import Model, Input, regularizers\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPool2D,Dropout, UpSampling2D, Conv2DTranspose,BatchNormalization,AveragePooling2D,ZeroPadding2D \n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import tensorflow\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import warnings;\n",
    "warnings.filterwarnings('ignore')\n",
    "from PIL import Image\n",
    "Image.MAX_IMAGE_PIXELS = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras.layers import InputLayer\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Reshape\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "from tensorflow.keras.applications.vgg16 import decode_predictions\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "# load an image from file\n",
    "import sys\n",
    "from numpy import load\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.image import imread\n",
    "from numpy import expand_dims\n",
    "\n",
    "import pickle\n",
    "from tensorflow.keras.preprocessing.image import array_to_img\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from numpy import expand_dims\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = tensorflow.keras.preprocessing.image.ImageDataGenerator(\n",
    "    #featurewise_center=True,\n",
    "    #featurewise_std_normalization=True,\n",
    "    #preprocessing_function=preprocess_input,\n",
    "    rescale=1/255)\n",
    "    #rotation_range=1,\n",
    "    #width_shift_range=1,\n",
    "    #height_shift_range=10,\n",
    "    #brightness_range=(.9,1.1),\n",
    "    #shear_range=0.8,\n",
    "    #zoom_range=[.8,1.1],\n",
    "    #channel_shift_range=.1,\n",
    "    #fill_mode=\"wrap\",\n",
    "    #horizontal_flip=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_datagen = tensorflow.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rescale=1/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 484 images belonging to 44 classes.\n"
     ]
    }
   ],
   "source": [
    "# setup generator\n",
    "train_generator = datagen.flow_from_directory(\n",
    "        directory=r\"C:\\Users\\dedunn\\Desktop\\full_aerin\",\n",
    "        target_size=(IMG_SIZE, IMG_SIZE),\n",
    "        color_mode='grayscale',\n",
    "        batch_size=1,\n",
    "        class_mode='input')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 484 images belonging to 44 classes.\n"
     ]
    }
   ],
   "source": [
    "true_generator = true_datagen.flow_from_directory(\n",
    "        directory=r\"C:\\Users\\dedunn\\Desktop\\full_aerin\",\n",
    "        target_size=(IMG_SIZE, IMG_SIZE),\n",
    "        color_mode='grayscale',\n",
    "        batch_size=1,\n",
    "        class_mode=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_generator =datagen.flow_from_directory(\n",
    "        directory=r\"C:\\Users\\dedunn\\Desktop\\full_aerin_test\",\n",
    "        target_size=(IMG_SIZE, IMG_SIZE),\n",
    "        color_mode='rgb',\n",
    "        batch_size=1,\n",
    "        class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_valid_generator =true_datagen.flow_from_directory(\n",
    "        directory=r\"C:\\Users\\dedunn\\Desktop\\full_aerin_test\",\n",
    "        target_size=(IMG_SIZE, IMG_SIZE),\n",
    "        color_mode='rgb',\n",
    "        batch_size=1,\n",
    "        class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 763,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE=80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for features, label in training_data:   \n",
    "    X.append(features)\n",
    "    y.append(label)\n",
    "#X=np.array(X).reshape(-1,IMG_SIZE,IMG_SIZE,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=X.reshape(-1,IMG_SIZE,IMG_SIZE,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat=os.listdir(r\"C:\\Users\\dedunn\\Desktop\\full_aerin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=load_img(r\"C:\\Users\\dedunn\\Desktop\\full_aerin\\AERIN_AM_MOODBOARD_LR.jpg\\AERIN_AM_MOODBOARD_LR.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 772,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 773,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image in range(len(cat)):\n",
    "    test=(r\"C:\\Users\\dedunn\\Desktop\\full_aerin\\{}\\{}\").format(cat[image],cat[image])\n",
    "    #test=load_img(r\"C:\\Users\\dedunn\\Desktop\\full_aerin\\{}\\{}\").format(cat[image],cat[image])\n",
    "    img_array = cv2.imread(test)#,cv2.IMREAD_GRAYSCALE)\n",
    "    new_array = cv2.resize(img_array,(IMG_SIZE,IMG_SIZE)).astype('float32')\n",
    "    test_image = new_array[...,::-1]\n",
    "    #test=load_img(test)\n",
    "    #dims=expand_dims(test,0)\n",
    "    #dims=cv2.resize(dims,(IMG_SIZE,IMG_SIZE)).astype('uint8')\n",
    "    training_data.append(test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 770,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28, 1)"
      ]
     },
     "execution_count": 770,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[8].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAIAAAD9b0jDAAAIQUlEQVR4nCXV+ZPWdR0A8Pfn+H6f7/N8n/vai91ld9llQV3CY0mBUYwJEdISdWw8yXIGzSYrymosx6MZcXIsrSjTkClkzAkBU0M5JhBwhQX3Ynfh2Xuf+/w+1/f6fD790OufeCEhhL04TZvbBK/YeoI6owgHASBfiPGFElFkonJMJCA+QmQLXLKhv/TbJ+57dCchBLSE7bS94T7MgNlMn4/jiMfmJhaMU38YOAihEewAvQQAACCKNpIkEVVYXaqgRqz4MCEOakohdUlLr+oJOlU/xVwAMCFMWxMgEEYIIduex6KYEYiaIsPqIexoEUIAAABnggIAS9scyU4EgMkPn3gCCAFMu7qXI864oetaHgEhDNB8jcoUGOPcBtKM4/G/VorHiR3EQIxEjiMCAqx4FEsEmRYGhBFDCAFC6wM2kVUAaIg2IkyoQ1EDYdWpcsbkNk0sZIRgyaFDUU8jJoV8NXOMFRdBAFZ82NVUjbeWF34gBYKiw4sQAkCSX8VIVEslQMCFOXbyKABwECS6MjMz4fQE+BQFf5zIovvGb2UreSwEZuUQCMyqaZAwN9Nq41Sw/xkrk6NI5bZFVAUJgQBuurZ78txnleJ4oZjgnOcL00UtG6xxCezSu4exFLg0sNeuI71Sx9SPKbe5n6GoEMWF+VQKiCQ4ggYhcgbxqNiDQQhESSqwVBLG9OBYLp+0beaSwyEXLQbgwtkh/3ceOnbg78saz1CXPHrsbQzAdGTGH/tXeiRj+cmeEzsB4Pkjd2uzhgnCtIRtuUFgBCSRymmldDqxEFuY8wUahMevQSVYazFnLlTlnNfvk/FmLMm3PvBLrKcl2+LC8oQjEZBQKpEtlgo3Lr1T9WPGmSQzWXJa3KoZ+rq1a+qF3NVNAQ7AdO3VF18bvmQnQhpVXJUTxzzeiOurP6tVqghLOLpsB6PVKj5O/BbV3K8/8nG5/upX/ns5mUjYLg94G7kEz3574+5/7h+YZyNHPjl0YsCwbQZmQwNtjWY7um5zmVr4ljvmzp8sPLVpJj0KALicecSy9Z4//m38579BQCbjbzhUH8u6PQ2HG0LBXDJdKWg793ywJP75LbdtXH7fU73rNxPBOLMvjs9Hq6ScTZgkpPFyz4bN1saHfIRJkoQR56FrnwRtesULv5h994Bby1maxh/WqMkkQjs72s1qdfqL4+mcUTp6uJJPTXz2wcqlfTJ3vPDkg5lTZxwUpNJiMNJSy6dSztHJiwMAgB3X/Dm9a5ADmtvyzLId91CJAjIBBOecCX5+30FMycndT5vp6UqhMHJo3/Zn/5QyK9948LH03Gho0/2SO4pX9CNR6d/yY/9197/0yntCIOxRlytjZXBKynNrpk8f8bdtkKfq6PCYAF0I0bphVHFI/TdtSGdTn+7/S7NkzEwNM938z8H90Z7lalh1WBUlymQ5whjSUou79/4eIUQBQCY8/sDz0q6tThGsiRBrf1BqN3X7S85BwuFsYnY8k+e2tmg6Q8yygP7k5X1aYSLY0EsITRsZV2MLoq5YajjQvpIQAsDxSOJkZP8OT1MTkaLg8xCJuTjBlCusa/zI6RoMVVLx7z675+SZ5I0btun1uqrgEKqHozdQ6qJWvcV7VUBWspU51eWhEjVMAxDFVzetVygE33wBSQQhwUxhYw5yu8sRCXdegnPZaM81mfzsltuve+Wdt4QA29CILGt6oZq/ggS5cPZAwTTCqkdwS9f1jnD3dHIQCyEAEABwJBBCGBBHmOs5d3MX8h1FBRaGWHFyTFacBz45JXsUxAKAkBcxJwdJUTq6WyKuzny57JUxsqxapa6VqlgIITgAAAIMABxhhATCMDh+bnLxMbuPWYLOJpKWcHz0j922xbpa/QiwsHXZG62XC7VsXa8bWl3Xa2lVuK8MH/344PvYMhkIEwAIICw4wgiEjewFZyLU/eL5fHbV2NBixOvMZeJLevpb77jv7LmhmpaQJFQtzhHiaOhcsTDxSdDdUhfS8O6HLx15/cO33qaxyR29V7+OEAEiAABxBizFzaClF0pIz+V62tpa792+DReLPaODGzZtzVQKRHbqRt3rDRYKMX2hJnO7kF/0B1pv3/WZkbm4ensVpRJXPB5Mldb87DlKKRCKEOIIcq/FfNgYuqo0cPS9tXfeZTB31Cd9dODgqjXrNn/zgVJmvpweUBx+yd0eG/6UcxZZvS1Rm3dkUN1VwzXdKsTfFwL9/zssgHOOuPBL9vjWufHLk7196zigXCKWTFW84Wi5kueWxayKu+H6os7UQGfX6jt7r99iVMvW+cst3V2nDu3DTx24hQCpJt9EWAAAIIExBsLkuuhcst3tCa9ad7NXapIIFmB976e/vqF/HeMmCGB6UpFZOTtq1rXTQ5+njNiCh385MbKqpxVNL45E/Q21/AdzJ6ynf/X9d06cQZRkKrOeN7TY3YabRadnJtqW9rVFcalYME1lxTU3GKZFqWxXNepWBSMzkwMXzp6qg7dv09p2lf774Bu4ralXlkn6d+d9a7Xnjgxknvn08NadU+OTLoaapM01nTa2XK+VysV8NdS6fPnKNTanrFZemBoUTl9yasQw66XsfA3KE6lYcUoRRM4XOZ3/8kfplxKtm0Chq0fz06nHh+44+Ghl2c31ox8Gl3gn0tmg2ubzeZs7Oi9euJDOnHAY5Zs33tXU3pecPR2IdOaTszPjX9z+0OO3FrOxy0a+oKz/+jasDM80f41lhlzJPxzvCLy32nqxdO/6dHzsyj1d+uN7HYKh6pzH6+G2LjmR2+9PFIlhVzFIDa3942fe94eaA2pk8HSFENIV9M3VcpJX+R/ZJn1Dpu+ZkAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=28x28 at 0x16E374A7E80>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array_to_img(training_data[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 774,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=np.array(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=X.reshape(-1,IMG_SIZE,IMG_SIZE,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=X/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 775,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44, 80, 80, 3)"
      ]
     },
     "execution_count": 775,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATADIR=r\"C:\\Users\\dedunn\\Desktop\\full_aerin\"\n",
    "CATEGORIES=cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_data():\n",
    "    for category in CATEGORIES:\n",
    "        path =  os.path.join(DATADIR,category)\n",
    "        class_num = CATEGORIES.index(category)\n",
    "        for img in os.listdir(path):\n",
    "            try:\n",
    "                img_array = cv2.imread(os.path.join(path,img),cv2.IMREAD_GRAYSCALE)\n",
    "                #img_array=img_to_array(img)\n",
    "                #img_array=expand_dims(img_array, 0)\n",
    "                new_array = cv2.resize(img_array,(IMG_SIZE,IMG_SIZE)).astype('float32')\n",
    "                test_image = img_array[...,::-1]\n",
    "                training_data.append([test_image,class_num])\n",
    "            except Exception as e:\n",
    "                pass\n",
    "create_training_data()\n",
    "#cv2.IMREAD_GRAYSCALE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "Input_img = tensorflow.keras.Input(shape=(28, 28, 1),dtype='float32')  \n",
    "    \n",
    "#encoding architecture\n",
    "x1 = Conv2D(256, (3, 3), activation='relu', padding='same')(Input_img)\n",
    "x2 = Conv2D(128, (3, 3), activation='relu', padding='same')(x1)\n",
    "x3 = MaxPool2D( (2, 2))(x2)\n",
    "encoded = Conv2D(64, (3, 3), activation='relu', padding='same')(x3)\n",
    "\n",
    "# decoding architecture\n",
    "x4 = Conv2D(64, (3, 3), activation='relu', padding='same')(encoded)\n",
    "x5 = UpSampling2D((2, 2))(x4)\n",
    "x6 = Conv2D(128, (3, 3), activation='relu', padding='same')(x5)\n",
    "x7 = Conv2D(256, (3, 3), activation='relu', padding='same')(x6)\n",
    "decoded = Conv2D(1, (3, 3),activation='sigmoid', padding='same')(x7)\n",
    "\n",
    "autoencoder = Model([Input_img, decoded])\n",
    "autoencoder.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=nb_train_samples,\n",
    "    epochs=epochs,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "('Error when checking model target: expected no data, but got:', array([[[[123],\n         [ 52],\n         [101],\n         ...,\n         [131],\n         [117],\n         [ 54]],\n\n        [[ 55],\n         [107],\n         [ 84],\n         ...,\n         [133],\n         [ 97],\n         [ 83]],\n\n        [[ 50],\n         [ 48],\n         [ 77],\n         ...,\n         [126],\n         [127],\n         [ 77]],\n\n        ...,\n\n        [[215],\n         [ 97],\n         [ 71],\n         ...,\n         [188],\n         [174],\n         [196]],\n\n        [[214],\n         [178],\n         [158],\n         ...,\n         [200],\n         [189],\n         [191]],\n\n        [[154],\n         [180],\n         [154],\n         ...,\n         [188],\n         [189],\n         [166]]],\n\n\n       [[[200],\n         [108],\n         [ 62],\n         ...,\n         [ 11],\n         [235],\n         [183]],\n\n        [[223],\n         [220],\n         [209],\n         ...,\n         [229],\n         [229],\n         [175]],\n\n        [[ 61],\n         [162],\n         [203],\n         ...,\n         [ 51],\n         [101],\n         [ 70]],\n\n        ...,\n\n        [[ 85],\n         [ 40],\n         [208],\n         ...,\n         [ 10],\n         [ 12],\n         [  8]],\n\n        [[156],\n         [145],\n         [158],\n         ...,\n         [ 33],\n         [ 26],\n         [ 36]],\n\n        [[136],\n         [ 33],\n         [113],\n         ...,\n         [131],\n         [178],\n         [132]]],\n\n\n       [[[ 40],\n         [ 35],\n         [ 67],\n         ...,\n         [165],\n         [ 60],\n         [ 29]],\n\n        [[ 48],\n         [ 20],\n         [ 69],\n         ...,\n         [160],\n         [ 86],\n         [ 53]],\n\n        [[ 59],\n         [ 17],\n         [158],\n         ...,\n         [235],\n         [ 39],\n         [ 40]],\n\n        ...,\n\n        [[ 88],\n         [  4],\n         [  9],\n         ...,\n         [ 32],\n         [ 17],\n         [ 14]],\n\n        [[ 10],\n         [ 23],\n         [  8],\n         ...,\n         [ 26],\n         [ 14],\n         [ 15]],\n\n        [[  3],\n         [141],\n         [ 12],\n         ...,\n         [ 27],\n         [ 26],\n         [ 17]]],\n\n\n       ...,\n\n\n       [[[206],\n         [208],\n         [208],\n         ...,\n         [209],\n         [208],\n         [208]],\n\n        [[203],\n         [203],\n         [204],\n         ...,\n         [211],\n         [213],\n         [211]],\n\n        [[206],\n         [207],\n         [204],\n         ...,\n         [199],\n         [198],\n         [196]],\n\n        ...,\n\n        [[ 83],\n         [ 57],\n         [ 57],\n         ...,\n         [163],\n         [173],\n         [170]],\n\n        [[ 79],\n         [ 82],\n         [ 67],\n         ...,\n         [208],\n         [194],\n         [211]],\n\n        [[ 88],\n         [200],\n         [134],\n         ...,\n         [115],\n         [ 68],\n         [139]]],\n\n\n       [[[  5],\n         [  5],\n         [  5],\n         ...,\n         [  5],\n         [  5],\n         [  5]],\n\n        [[  5],\n         [  5],\n         [  5],\n         ...,\n         [  5],\n         [  5],\n         [  4]],\n\n        [[  5],\n         [  5],\n         [  4],\n         ...,\n         [  5],\n         [  5],\n         [  5]],\n\n        ...,\n\n        [[  5],\n         [  5],\n         [  5],\n         ...,\n         [  5],\n         [  4],\n         [  4]],\n\n        [[  5],\n         [  5],\n         [  5],\n         ...,\n         [  5],\n         [  5],\n         [  5]],\n\n        [[  5],\n         [  5],\n         [  5],\n         ...,\n         [  5],\n         [  5],\n         [  4]]],\n\n\n       [[[212],\n         [210],\n         [211],\n         ...,\n         [224],\n         [223],\n         [222]],\n\n        [[216],\n         [213],\n         [215],\n         ...,\n         [225],\n         [226],\n         [225]],\n\n        [[222],\n         [221],\n         [221],\n         ...,\n         [229],\n         [227],\n         [228]],\n\n        ...,\n\n        [[240],\n         [239],\n         [113],\n         ...,\n         [234],\n         [231],\n         [236]],\n\n        [[239],\n         [240],\n         [227],\n         ...,\n         [230],\n         [231],\n         [230]],\n\n        [[238],\n         [240],\n         [229],\n         ...,\n         [231],\n         [237],\n         [232]]]], dtype=uint8))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-125-029e05f96042>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m             \u001b[1;31m#shuffle=True,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m             \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m44\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m             validation_data=(X, X))\n\u001b[0m\u001b[0;32m      8\u001b[0m             \u001b[1;31m#callbacks=[early_stopper])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    233\u001b[0m           \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    234\u001b[0m           \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 235\u001b[1;33m           use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    236\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    237\u001b[0m       \u001b[0mtotal_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_total_number_of_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_data_adapter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36m_process_training_inputs\u001b[1;34m(model, x, y, batch_size, epochs, sample_weights, class_weights, steps_per_epoch, validation_split, validation_data, validation_steps, shuffle, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    591\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    592\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 593\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    594\u001b[0m     \u001b[0mval_adapter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    595\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36m_process_inputs\u001b[1;34m(model, mode, x, y, batch_size, epochs, sample_weights, class_weights, shuffle, steps, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    644\u001b[0m     \u001b[0mstandardize_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    645\u001b[0m     x, y, sample_weights = standardize(\n\u001b[1;32m--> 646\u001b[1;33m         x, y, sample_weight=sample_weights)\n\u001b[0m\u001b[0;32m    647\u001b[0m   \u001b[1;32melif\u001b[0m \u001b[0madapter_cls\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mdata_adapter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mListsOfScalarsDataAdapter\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    648\u001b[0m     \u001b[0mstandardize_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstandardize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[0;32m   2381\u001b[0m         \u001b[0mis_dataset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mis_dataset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2382\u001b[0m         \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2383\u001b[1;33m         batch_size=batch_size)\n\u001b[0m\u001b[0;32m   2384\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2385\u001b[0m   def _standardize_tensors(self, x, y, sample_weight, run_eagerly, dict_inputs,\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_tensors\u001b[1;34m(self, x, y, sample_weight, run_eagerly, dict_inputs, is_dataset, class_weight, batch_size)\u001b[0m\n\u001b[0;32m   2467\u001b[0m           \u001b[0mshapes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2468\u001b[0m           \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2469\u001b[1;33m           exception_prefix='target')\n\u001b[0m\u001b[0;32m   2470\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2471\u001b[0m       \u001b[1;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    494\u001b[0m       raise ValueError(\n\u001b[0;32m    495\u001b[0m           \u001b[1;34m'Error when checking model '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mexception_prefix\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m': '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 496\u001b[1;33m           'expected no data, but got:', data)\n\u001b[0m\u001b[0;32m    497\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    498\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: ('Error when checking model target: expected no data, but got:', array([[[[123],\n         [ 52],\n         [101],\n         ...,\n         [131],\n         [117],\n         [ 54]],\n\n        [[ 55],\n         [107],\n         [ 84],\n         ...,\n         [133],\n         [ 97],\n         [ 83]],\n\n        [[ 50],\n         [ 48],\n         [ 77],\n         ...,\n         [126],\n         [127],\n         [ 77]],\n\n        ...,\n\n        [[215],\n         [ 97],\n         [ 71],\n         ...,\n         [188],\n         [174],\n         [196]],\n\n        [[214],\n         [178],\n         [158],\n         ...,\n         [200],\n         [189],\n         [191]],\n\n        [[154],\n         [180],\n         [154],\n         ...,\n         [188],\n         [189],\n         [166]]],\n\n\n       [[[200],\n         [108],\n         [ 62],\n         ...,\n         [ 11],\n         [235],\n         [183]],\n\n        [[223],\n         [220],\n         [209],\n         ...,\n         [229],\n         [229],\n         [175]],\n\n        [[ 61],\n         [162],\n         [203],\n         ...,\n         [ 51],\n         [101],\n         [ 70]],\n\n        ...,\n\n        [[ 85],\n         [ 40],\n         [208],\n         ...,\n         [ 10],\n         [ 12],\n         [  8]],\n\n        [[156],\n         [145],\n         [158],\n         ...,\n         [ 33],\n         [ 26],\n         [ 36]],\n\n        [[136],\n         [ 33],\n         [113],\n         ...,\n         [131],\n         [178],\n         [132]]],\n\n\n       [[[ 40],\n         [ 35],\n         [ 67],\n         ...,\n         [165],\n         [ 60],\n         [ 29]],\n\n        [[ 48],\n         [ 20],\n         [ 69],\n         ...,\n         [160],\n         [ 86],\n         [ 53]],\n\n        [[ 59],\n         [ 17],\n         [158],\n         ...,\n         [235],\n         [ 39],\n         [ 40]],\n\n        ...,\n\n        [[ 88],\n         [  4],\n         [  9],\n         ...,\n         [ 32],\n         [ 17],\n         [ 14]],\n\n        [[ 10],\n         [ 23],\n         [  8],\n         ...,\n         [ 26],\n         [ 14],\n         [ 15]],\n\n        [[  3],\n         [141],\n         [ 12],\n         ...,\n         [ 27],\n         [ 26],\n         [ 17]]],\n\n\n       ...,\n\n\n       [[[206],\n         [208],\n         [208],\n         ...,\n         [209],\n         [208],\n         [208]],\n\n        [[203],\n         [203],\n         [204],\n         ...,\n         [211],\n         [213],\n         [211]],\n\n        [[206],\n         [207],\n         [204],\n         ...,\n         [199],\n         [198],\n         [196]],\n\n        ...,\n\n        [[ 83],\n         [ 57],\n         [ 57],\n         ...,\n         [163],\n         [173],\n         [170]],\n\n        [[ 79],\n         [ 82],\n         [ 67],\n         ...,\n         [208],\n         [194],\n         [211]],\n\n        [[ 88],\n         [200],\n         [134],\n         ...,\n         [115],\n         [ 68],\n         [139]]],\n\n\n       [[[  5],\n         [  5],\n         [  5],\n         ...,\n         [  5],\n         [  5],\n         [  5]],\n\n        [[  5],\n         [  5],\n         [  5],\n         ...,\n         [  5],\n         [  5],\n         [  4]],\n\n        [[  5],\n         [  5],\n         [  4],\n         ...,\n         [  5],\n         [  5],\n         [  5]],\n\n        ...,\n\n        [[  5],\n         [  5],\n         [  5],\n         ...,\n         [  5],\n         [  4],\n         [  4]],\n\n        [[  5],\n         [  5],\n         [  5],\n         ...,\n         [  5],\n         [  5],\n         [  5]],\n\n        [[  5],\n         [  5],\n         [  5],\n         ...,\n         [  5],\n         [  5],\n         [  4]]],\n\n\n       [[[212],\n         [210],\n         [211],\n         ...,\n         [224],\n         [223],\n         [222]],\n\n        [[216],\n         [213],\n         [215],\n         ...,\n         [225],\n         [226],\n         [225]],\n\n        [[222],\n         [221],\n         [221],\n         ...,\n         [229],\n         [227],\n         [228]],\n\n        ...,\n\n        [[240],\n         [239],\n         [113],\n         ...,\n         [234],\n         [231],\n         [236]],\n\n        [[239],\n         [240],\n         [227],\n         ...,\n         [230],\n         [231],\n         [230]],\n\n        [[238],\n         [240],\n         [229],\n         ...,\n         [231],\n         [237],\n         [232]]]], dtype=uint8))"
     ]
    }
   ],
   "source": [
    "early_stopper = EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=4, verbose=1, mode='auto')\n",
    "\n",
    "a_e = autoencoder.fit(X,X,\n",
    "            epochs=50,\n",
    "            #shuffle=True,\n",
    "            steps_per_epoch=44,\n",
    "            validation_data=(X, X))\n",
    "            #callbacks=[early_stopper])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_autoencoder(img_shape, code_size):\n",
    "    # The encoder\n",
    "    encoder = Sequential()\n",
    "    encoder.add(InputLayer(img_shape))\n",
    "    encoder.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
    "    encoder.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "    encoder.add(MaxPool2D( (2, 2)))\n",
    "    #encoder.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "    encoder.add(Flatten())\n",
    "    encoder.add(Dense(code_size))\n",
    "\n",
    "    # The decoder\n",
    "    decoder = Sequential()\n",
    "    decoder.add(InputLayer((code_size)))\n",
    "    #decoder.add(Reshape(img_shape))\n",
    "    decoder.add(UpSampling2D(interpolation='nearest'))\n",
    "    decoder.add(Reshape(img_shape))\n",
    "    decoder.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "    decoder.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "    #decoder.add(UpSampling2D((2,2)))\n",
    "    #decoder.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "    decoder.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
    "    decoder.add(Flatten())\n",
    "    #decoder.add(UpSampling2D(2,2))\n",
    "    decoder.add(Dense(np.prod(img_shape))) # np.prod(img_shape) is the same as 32*32*3, it's more generic than saying 3072\n",
    "    decoder.add(Reshape(img_shape))\n",
    "    #decoder.add(Conv2D(1, (3, 3), activation='sigmoid', padding='same'))\n",
    "    return encoder, decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(encoder.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(decoder.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input 0 of layer up_sampling2d_4 is incompatible with the layer: expected ndim=4, found ndim=2. Full shape received: [None, 32]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-fe33cca62373>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mIMG_SHAPE\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mencoder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecoder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_autoencoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mIMG_SHAPE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0minp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mIMG_SHAPE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mcode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-26-eea2cacbbc9c>\u001b[0m in \u001b[0;36mbuild_autoencoder\u001b[1;34m(img_shape, code_size)\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mdecoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mInputLayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcode_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;31m#decoder.add(Reshape(img_shape))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[0mdecoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mUpSampling2D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'nearest'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m     \u001b[0mdecoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mReshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mdecoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mConv2D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'relu'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'same'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow_core\\python\\training\\tracking\\base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    455\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\sequential.py\u001b[0m in \u001b[0;36madd\u001b[1;34m(self, layer)\u001b[0m\n\u001b[0;32m    201\u001b[0m       \u001b[1;31m# If the model is being built continuously on top of an input layer:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    202\u001b[0m       \u001b[1;31m# refresh its output.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m       \u001b[0moutput_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    204\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m         raise TypeError('All layers in a Sequential model '\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    735\u001b[0m         \u001b[1;31m# are casted, not before.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    736\u001b[0m         input_spec.assert_input_compatibility(self.input_spec, inputs,\n\u001b[1;32m--> 737\u001b[1;33m                                               self.name)\n\u001b[0m\u001b[0;32m    738\u001b[0m         if (any(isinstance(x, ragged_tensor.RaggedTensor) for x in input_list)\n\u001b[0;32m    739\u001b[0m             and self._supports_ragged_inputs is False):  # pylint: disable=g-bool-id-comparison\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\input_spec.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[1;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[0;32m    175\u001b[0m                          \u001b[1;34m'expected ndim='\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m', found ndim='\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    176\u001b[0m                          \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'. Full shape received: '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 177\u001b[1;33m                          str(x.shape.as_list()))\n\u001b[0m\u001b[0;32m    178\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mspec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_ndim\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m       \u001b[0mndim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndims\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Input 0 of layer up_sampling2d_4 is incompatible with the layer: expected ndim=4, found ndim=2. Full shape received: [None, 32]"
     ]
    }
   ],
   "source": [
    "IMG_SHAPE = X.shape[1:]\n",
    "encoder, decoder = build_autoencoder(IMG_SHAPE, 32)\n",
    "\n",
    "inp = Input(IMG_SHAPE)\n",
    "code = encoder(inp)\n",
    "reconstruction = decoder(code)\n",
    "\n",
    "autoencoder = Model(inp,reconstruction)\n",
    "autoencoder.compile(optimizer='adamax', loss='mse')\n",
    "\n",
    "print(autoencoder.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 44 samples, validate on 44 samples\n",
      "Epoch 1/20000\n",
      "\r",
      "32/44 [====================>.........] - ETA: 0s"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": " Input to reshape is a tensor with 1024 values, but the requested shape has 25088\n\t [[node model_3/sequential_7/reshape_6/Reshape (defined at <ipython-input-24-5cd3186f7957>:2) ]] [Op:__inference_distributed_function_5139]\n\nFunction call stack:\ndistributed_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-5cd3186f7957>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m history = autoencoder.fit(x=X, y=X, epochs=20000,\n\u001b[1;32m----> 2\u001b[1;33m                 validation_data=[X, X])\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    340\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 342\u001b[1;33m                 total_epochs=epochs)\n\u001b[0m\u001b[0;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[1;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[0;32m    126\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[0;32m    127\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m         \u001b[1;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[1;34m(input_fn)\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[1;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[1;32m---> 98\u001b[1;33m                               distributed_function(input_fn))\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    567\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 568\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    569\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    570\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    597\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    598\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 599\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    600\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    601\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2361\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2363\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2365\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1609\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[0;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m-> 1611\u001b[1;33m         self.captured_inputs)\n\u001b[0m\u001b[0;32m   1612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1613\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1690\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1692\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m     \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m     keras_symbolic_tensors = [\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\six.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m:  Input to reshape is a tensor with 1024 values, but the requested shape has 25088\n\t [[node model_3/sequential_7/reshape_6/Reshape (defined at <ipython-input-24-5cd3186f7957>:2) ]] [Op:__inference_distributed_function_5139]\n\nFunction call stack:\ndistributed_function\n"
     ]
    }
   ],
   "source": [
    "history = autoencoder.fit(x=X, y=X, epochs=20000,\n",
    "                validation_data=[X, X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_12 (Conv2D)           (None, 28, 28, 256)       2560      \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 28, 28, 128)       295040    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 32)                802848    \n",
      "=================================================================\n",
      "Total params: 1,100,448\n",
      "Trainable params: 1,100,448\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "reshape_6 (Reshape)          (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_3 (UpSampling2 (None, 56, 56, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, 56, 56, 64)        640       \n",
      "_________________________________________________________________\n",
      "conv2d_20 (Conv2D)           (None, 56, 56, 128)       73856     \n",
      "_________________________________________________________________\n",
      "conv2d_21 (Conv2D)           (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 802816)            0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 784)               629408528 \n",
      "_________________________________________________________________\n",
      "reshape_7 (Reshape)          (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 629,778,192\n",
      "Trainable params: 629,778,192\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         [(None, 56, 56, 1)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 56, 56, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 28, 28, 32)        128       \n",
      "_________________________________________________________________\n",
      "average_pooling2d_4 (Average (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 128)               802944    \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 16)                1040      \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 1568)              26656     \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 3136)              4920384   \n",
      "_________________________________________________________________\n",
      "reshape_4 (Reshape)          (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_16 (Conv2DT (None, 14, 14, 64)        36928     \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_17 (Conv2DT (None, 28, 28, 128)       73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 28, 28, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_18 (Conv2DT (None, 56, 56, 64)        73792     \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_19 (Conv2DT (None, 56, 56, 32)        18464     \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 56, 56, 1)         289       \n",
      "=================================================================\n",
      "Total params: 5,963,569\n",
      "Trainable params: 5,963,249\n",
      "Non-trainable params: 320\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#ENCODER\n",
    "#kernel_initializer='he_uniform' for relu\n",
    "inp = Input((56, 56,1))\n",
    "e = Conv2D(32, (3, 3), strides=1,activation='relu',padding='same')(inp)\n",
    "#e = Conv2D(64, (3, 3), strides=2,activation='relu',padding='same')(e)\n",
    "e = MaxPool2D((2, 2))(e)\n",
    "#e = Conv2D(64, (3, 3),strides=2,padding='same', activation='relu')(e)\n",
    "e = BatchNormalization()(e)\n",
    "#e = MaxPool2D((2, 2))(e)\n",
    "#e = Conv2D(64, (3, 3),strides=2, activation='relu',padding='same')(e)\n",
    "e = AveragePooling2D((2,2))(e)\n",
    "l = Flatten()(e)\n",
    "l = Dense(128, activation='relu')(l)\n",
    "l = Dense(64, activation='relu')(l)\n",
    "l = Dense(16, activation='relu')(l)\n",
    "#DECODER\n",
    "d =Dense(784, activation='relu')(l)\n",
    "d =Dense(1568, activation='relu')(l)\n",
    "d =Dense(3136, activation='relu')(d)\n",
    "d = Reshape((7,7,64))(d)\n",
    "#d = ZeroPadding2D(padding=((0,0),(0,1)))(d)\n",
    "#d = UpSampling2D()(d)\n",
    "#d = Reshape((14,14,1))(d)\n",
    "d = Conv2DTranspose(64,(3, 3), strides=2, activation='relu', padding='same')(d)\n",
    "d = Conv2DTranspose(128,(3, 3), strides =2,activation='relu', padding='same')(d)\n",
    "d = BatchNormalization()(d)\n",
    "#d = UpSampling2D()(d)\n",
    "#d = Conv2DTranspose(64,(3, 3), strides=2, activation='relu', padding='same')(d)\n",
    "#d = BatchNormalization()(d)\n",
    "d = Conv2DTranspose(64,(3, 3),strides=2, activation='relu', padding='same')(d)\n",
    "d = Conv2DTranspose(32,(3, 3), activation='relu', padding='same')(d)\n",
    "#d = Reshape((28,28,1))(d)\n",
    "#d = Dense(32, activation='relu')(d)\n",
    "decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(d)\n",
    "ae = Model(inp, decoded)\n",
    "ae.summary()\n",
    "#Input to reshape is a tensor with 1408 values, but the requested shape has 1344"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# compile it using adam optimizer\n",
    "#ae.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "#Train it by providing training images\n",
    "#ae.fit(X, X, epochs=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=28x28 at 0x16E42440320>"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array_to_img(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28, 1)"
      ]
     },
     "execution_count": 456,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 826,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y=X/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 738,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_img = Input(shape=(28, 28, 1))  # adapt this if using `channels_first` image data format\n",
    "\n",
    "x = Conv2D(128, (3,3 ), activation='relu', padding='same')(input_img)\n",
    "x = MaxPool2D((2, 2), padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(256, (3, 3),strides=(2,2), activation='relu', padding='same')(x)\n",
    "x = MaxPool2D((2, 2), padding='same')(x)\n",
    "#x = Dropout(.5)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(256, (3, 3),strides=(1,1), activation='elu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = AveragePooling2D((3, 3), padding='same')(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(256, activation = 'relu')(x)\n",
    "encoded = Dense(128, activation = 'relu')(x)\n",
    "#encoded = Dense(49, activation = 'relu')(x)\n",
    "#encoded = Dense(512,activation='elu')(x)\n",
    "# at this point the representation is (4, 4, 8) i.e. 128-dimensional\n",
    "#x = Dense(196, activation = 'elu')(encoded)\n",
    "x= Reshape((4,4,8))(encoded)\n",
    "#x = Conv2D(128, (3, 3),strides=(1,1), activation='elu', padding='same')(x)\n",
    "x = Conv2DTranspose(128, (3, 3),strides=(2,2),activation='relu',padding='same' )(x)\n",
    "x = BatchNormalization()(x)\n",
    "#x = Conv2D(64, (3, 3),strides=(1,1), activation='elu', padding='same')(x)\n",
    "x = Conv2DTranspose(256, (3, 3),strides=(2,2), activation='relu',padding='same' )(x)\n",
    "x = BatchNormalization()(x)\n",
    "#x = UpSampling2D((2, 2))(x)\n",
    "#x = Conv2D(32, (3, 3), activation='elu',padding='same')(x)\n",
    "#x = BatchNormalization()(x)\n",
    "#x = Conv2D(32, (3, 3),activation='elu')(x)\n",
    "#x = BatchNormalization()(x)\n",
    "#x = UpSampling2D((2, 2))(x)\n",
    "x = Conv2DTranspose(256, (3, 3),strides=(2,2), activation='elu',padding='same')(x)\n",
    "#x = BatchNormalization()(x)\n",
    "#x = UpSampling2D((2, 2))(x)\n",
    "#x= Reshape((28,28,3))(x)\n",
    "decoded = Conv2D(1, (5, 5), activation='elu')(x)\n",
    "\n",
    "autoencoder = Model(input_img, decoded)\n",
    "autoencoder.compile(optimizer='adam', loss=\"mse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 758,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_img = Input(shape=(28, 28, 1))\n",
    "#x = Conv2D(128, (3,3 ), activation='relu', padding='same')(input_img)\n",
    "#x = MaxPool2D((2, 2), padding='same')(x)\n",
    "#x = BatchNormalization()(x)\n",
    "x = Flatten()(input_img)\n",
    "x = Dense(785, activation = 'relu')(x)\n",
    "x = Dense(390, activation = 'relu')(x)\n",
    "x = Dense(150, activation = 'relu')(x)\n",
    "encoded = Dense(44, activation = 'relu')(x)\n",
    "x = Dense(150, activation = 'relu')(encoded)\n",
    "x = Dense(390, activation = 'relu')(x)\n",
    "decoded = Dense(785, activation = 'sigmoid')(x)\n",
    "#x= Reshape((28,28,1))(x)\n",
    "#x = Conv2DTranspose(128, (3, 3),strides=(2,2),activation='relu',padding='same' )(x)\n",
    "#decoded = Conv2D(1, (1, 1),strides=(1,1), activation='sigmoid')(x)\n",
    "autoencoder = Model(input_img, decoded)\n",
    "autoencoder.compile(optimizer='adam', loss=\"mse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 823,
   "metadata": {},
   "outputs": [],
   "source": [
    "Input_img = Input(shape=(80, 80, 3))  \n",
    "    \n",
    "#encoding architecture\n",
    "x1 = Conv2D(64, (3, 3), activation='relu', padding='same')(Input_img)\n",
    "x1 = BatchNormalization()(x1)\n",
    "x2 = Conv2D(128, (3, 3), activation='relu', padding='same')(x1)\n",
    "x2 = MaxPool2D( (2, 2))(x2)\n",
    "x2 = BatchNormalization()(x2)\n",
    "x2 = Conv2D(64, (3, 3), activation='relu', padding='same')(x2)\n",
    "encoded = AveragePooling2D( (2, 2))(x2)\n",
    "\n",
    "# decoding architecture\n",
    "x3=Conv2D(8, (3, 3), activation='relu', padding='same')(encoded)\n",
    "x3 = Conv2DTranspose(64, (5, 5),strides=2, activation='relu', padding='same')(x3)\n",
    "x3 = Conv2DTranspose(128, (3, 3),strides=(2,2),activation='relu',padding='same' )(x3)\n",
    "#x3 = UpSampling2D((2, 2))(x3)\n",
    "#x2 = Conv2D(128, (3, 3), activation='relu', padding='same')(x3)\n",
    "x1 = Conv2DTranspose(256, (3, 3), activation='relu', padding='same')(x3)\n",
    "decoded = Conv2D(3, (1, 1), padding='same')(x1)\n",
    "\n",
    "autoencoder = Model(Input_img, decoded)\n",
    "autoencoder.compile(optimizer='Nadam', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 824,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_249\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_277 (InputLayer)       [(None, 80, 80, 3)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1138 (Conv2D)         (None, 80, 80, 64)        1792      \n",
      "_________________________________________________________________\n",
      "batch_normalization_792 (Bat (None, 80, 80, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_1139 (Conv2D)         (None, 80, 80, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_356 (MaxPoolin (None, 40, 40, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_793 (Bat (None, 40, 40, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv2d_1140 (Conv2D)         (None, 40, 40, 64)        73792     \n",
      "_________________________________________________________________\n",
      "average_pooling2d_199 (Avera (None, 20, 20, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1141 (Conv2D)         (None, 20, 20, 8)         4616      \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_585 (Conv2D (None, 40, 40, 64)        12864     \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_586 (Conv2D (None, 80, 80, 128)       73856     \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_587 (Conv2D (None, 80, 80, 256)       295168    \n",
      "_________________________________________________________________\n",
      "conv2d_1142 (Conv2D)         (None, 80, 80, 3)         771       \n",
      "=================================================================\n",
      "Total params: 537,483\n",
      "Trainable params: 537,099\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 44 steps\n",
      "Epoch 1/50000\n",
      "44/44 [==============================] - 8s 185ms/step - loss: 2145.2652\n",
      "Epoch 2/50000\n",
      "44/44 [==============================] - 8s 193ms/step - loss: 2.3925\n",
      "Epoch 3/50000\n",
      "44/44 [==============================] - 8s 190ms/step - loss: 1.2571\n",
      "Epoch 4/50000\n",
      "44/44 [==============================] - 9s 198ms/step - loss: 0.8703\n",
      "Epoch 5/50000\n",
      "44/44 [==============================] - 9s 198ms/step - loss: 0.6797\n",
      "Epoch 6/50000\n",
      "44/44 [==============================] - 9s 199ms/step - loss: 0.5646\n",
      "Epoch 7/50000\n",
      "44/44 [==============================] - 9s 195ms/step - loss: 0.4853\n",
      "Epoch 8/50000\n",
      "44/44 [==============================] - 9s 196ms/step - loss: 0.4299\n",
      "Epoch 9/50000\n",
      "44/44 [==============================] - 9s 199ms/step - loss: 0.3830\n",
      "Epoch 10/50000\n",
      "44/44 [==============================] - 9s 201ms/step - loss: 0.3540\n",
      "Epoch 11/50000\n",
      "44/44 [==============================] - 9s 211ms/step - loss: 0.3231\n",
      "Epoch 12/50000\n",
      "44/44 [==============================] - 9s 209ms/step - loss: 0.3011\n",
      "Epoch 13/50000\n",
      "44/44 [==============================] - 9s 204ms/step - loss: 0.2813\n",
      "Epoch 14/50000\n",
      "44/44 [==============================] - 9s 204ms/step - loss: 0.2651\n",
      "Epoch 15/50000\n",
      "44/44 [==============================] - 9s 204ms/step - loss: 0.2476\n",
      "Epoch 16/50000\n",
      "44/44 [==============================] - 9s 204ms/step - loss: 0.2411\n",
      "Epoch 17/50000\n",
      "44/44 [==============================] - 9s 206ms/step - loss: 0.2270\n",
      "Epoch 18/50000\n",
      "44/44 [==============================] - 9s 210ms/step - loss: 0.2187\n",
      "Epoch 19/50000\n",
      "44/44 [==============================] - 9s 207ms/step - loss: 0.2063\n",
      "Epoch 20/50000\n",
      "44/44 [==============================] - 9s 208ms/step - loss: 0.1972\n",
      "Epoch 21/50000\n",
      "44/44 [==============================] - 9s 211ms/step - loss: 0.1924\n",
      "Epoch 22/50000\n",
      "44/44 [==============================] - 9s 210ms/step - loss: 0.1832\n",
      "Epoch 23/50000\n",
      "44/44 [==============================] - 9s 212ms/step - loss: 0.1774\n",
      "Epoch 24/50000\n",
      "44/44 [==============================] - 9s 214ms/step - loss: 0.1732\n",
      "Epoch 25/50000\n",
      "44/44 [==============================] - 10s 216ms/step - loss: 0.1657\n",
      "Epoch 26/50000\n",
      "44/44 [==============================] - 10s 217ms/step - loss: 0.1610\n",
      "Epoch 27/50000\n",
      "44/44 [==============================] - 10s 220ms/step - loss: 0.1563\n",
      "Epoch 28/50000\n",
      "44/44 [==============================] - 10s 222ms/step - loss: 0.1559\n",
      "Epoch 29/50000\n",
      "44/44 [==============================] - 10s 217ms/step - loss: 0.1477\n",
      "Epoch 30/50000\n",
      "44/44 [==============================] - 10s 219ms/step - loss: 0.1406\n",
      "Epoch 31/50000\n",
      "44/44 [==============================] - 10s 222ms/step - loss: 0.1399\n",
      "Epoch 32/50000\n",
      "44/44 [==============================] - 10s 216ms/step - loss: 0.1405\n",
      "Epoch 33/50000\n",
      "44/44 [==============================] - 10s 217ms/step - loss: 0.1318\n",
      "Epoch 34/50000\n",
      "44/44 [==============================] - 10s 216ms/step - loss: 0.1316\n",
      "Epoch 35/50000\n",
      "44/44 [==============================] - 9s 215ms/step - loss: 0.1252\n",
      "Epoch 36/50000\n",
      "44/44 [==============================] - 10s 218ms/step - loss: 0.1243\n",
      "Epoch 37/50000\n",
      "44/44 [==============================] - 10s 220ms/step - loss: 0.1217\n",
      "Epoch 38/50000\n",
      "44/44 [==============================] - 9s 216ms/step - loss: 0.1206\n",
      "Epoch 39/50000\n",
      "44/44 [==============================] - 10s 219ms/step - loss: 0.1194\n",
      "Epoch 40/50000\n",
      "44/44 [==============================] - 10s 216ms/step - loss: 0.1153\n",
      "Epoch 41/50000\n",
      "44/44 [==============================] - 10s 217ms/step - loss: 0.1144\n",
      "Epoch 42/50000\n",
      "44/44 [==============================] - 10s 218ms/step - loss: 0.1123\n",
      "Epoch 43/50000\n",
      "44/44 [==============================] - 10s 219ms/step - loss: 0.1103\n",
      "Epoch 44/50000\n",
      "44/44 [==============================] - 10s 223ms/step - loss: 0.1085\n",
      "Epoch 45/50000\n",
      "44/44 [==============================] - 10s 218ms/step - loss: 0.1093\n",
      "Epoch 46/50000\n",
      "44/44 [==============================] - 10s 219ms/step - loss: 0.1054\n",
      "Epoch 47/50000\n",
      "44/44 [==============================] - 10s 227ms/step - loss: 0.1048\n",
      "Epoch 48/50000\n",
      "44/44 [==============================] - 10s 216ms/step - loss: 0.1039\n",
      "Epoch 49/50000\n",
      "44/44 [==============================] - 10s 221ms/step - loss: 0.1018\n",
      "Epoch 50/50000\n",
      "44/44 [==============================] - 10s 221ms/step - loss: 0.1008\n",
      "Epoch 51/50000\n",
      "44/44 [==============================] - 10s 219ms/step - loss: 0.0994\n",
      "Epoch 52/50000\n",
      "44/44 [==============================] - 10s 220ms/step - loss: 0.0994\n",
      "Epoch 53/50000\n",
      "44/44 [==============================] - 10s 218ms/step - loss: 0.0973\n",
      "Epoch 54/50000\n",
      "44/44 [==============================] - 10s 227ms/step - loss: 0.0994\n",
      "Epoch 55/50000\n",
      "44/44 [==============================] - 10s 219ms/step - loss: 0.0980\n",
      "Epoch 56/50000\n",
      "44/44 [==============================] - 10s 219ms/step - loss: 0.0968\n",
      "Epoch 57/50000\n",
      "44/44 [==============================] - 10s 218ms/step - loss: 0.0941\n",
      "Epoch 58/50000\n",
      "44/44 [==============================] - 10s 219ms/step - loss: 0.0922\n",
      "Epoch 59/50000\n",
      "44/44 [==============================] - 10s 221ms/step - loss: 0.0919\n",
      "Epoch 60/50000\n",
      "44/44 [==============================] - 10s 218ms/step - loss: 0.0907\n",
      "Epoch 61/50000\n",
      "44/44 [==============================] - 10s 221ms/step - loss: 0.0886\n",
      "Epoch 62/50000\n",
      "44/44 [==============================] - 10s 219ms/step - loss: 0.0913\n",
      "Epoch 63/50000\n",
      "44/44 [==============================] - 10s 221ms/step - loss: 0.0910\n",
      "Epoch 64/50000\n",
      "44/44 [==============================] - 10s 219ms/step - loss: 0.0878\n",
      "Epoch 65/50000\n",
      "44/44 [==============================] - 10s 221ms/step - loss: 0.0878\n",
      "Epoch 66/50000\n",
      "44/44 [==============================] - 10s 219ms/step - loss: 0.0862\n",
      "Epoch 67/50000\n",
      "44/44 [==============================] - 10s 218ms/step - loss: 0.0860\n",
      "Epoch 68/50000\n",
      "44/44 [==============================] - 10s 219ms/step - loss: 0.0854\n",
      "Epoch 69/50000\n",
      "44/44 [==============================] - 10s 221ms/step - loss: 0.0844\n",
      "Epoch 70/50000\n",
      "44/44 [==============================] - 10s 223ms/step - loss: 0.0839\n",
      "Epoch 71/50000\n",
      "44/44 [==============================] - 10s 222ms/step - loss: 0.0849\n",
      "Epoch 72/50000\n",
      "44/44 [==============================] - 10s 218ms/step - loss: 0.0825\n",
      "Epoch 73/50000\n",
      "44/44 [==============================] - 10s 226ms/step - loss: 0.0817\n",
      "Epoch 74/50000\n",
      "44/44 [==============================] - 10s 219ms/step - loss: 0.0821\n",
      "Epoch 75/50000\n",
      "44/44 [==============================] - 10s 224ms/step - loss: 0.0788\n",
      "Epoch 76/50000\n",
      "44/44 [==============================] - 10s 217ms/step - loss: 0.0789\n",
      "Epoch 77/50000\n",
      "44/44 [==============================] - 10s 221ms/step - loss: 0.0804\n",
      "Epoch 78/50000\n",
      "44/44 [==============================] - 10s 217ms/step - loss: 0.0782\n",
      "Epoch 79/50000\n",
      "44/44 [==============================] - 10s 218ms/step - loss: 0.0796\n",
      "Epoch 80/50000\n",
      "44/44 [==============================] - 10s 219ms/step - loss: 0.0781\n",
      "Epoch 81/50000\n",
      "44/44 [==============================] - 10s 220ms/step - loss: 0.0786\n",
      "Epoch 82/50000\n",
      "44/44 [==============================] - 10s 220ms/step - loss: 0.0766\n",
      "Epoch 83/50000\n",
      "44/44 [==============================] - 10s 220ms/step - loss: 0.0756\n",
      "Epoch 84/50000\n",
      "44/44 [==============================] - 10s 219ms/step - loss: 0.0760\n",
      "Epoch 85/50000\n",
      "44/44 [==============================] - 10s 218ms/step - loss: 0.0759\n",
      "Epoch 86/50000\n",
      "44/44 [==============================] - 10s 222ms/step - loss: 0.0731\n",
      "Epoch 87/50000\n",
      "44/44 [==============================] - 10s 219ms/step - loss: 0.0727\n",
      "Epoch 88/50000\n",
      "44/44 [==============================] - 10s 231ms/step - loss: 0.0732\n",
      "Epoch 89/50000\n",
      "44/44 [==============================] - 10s 220ms/step - loss: 0.0726\n",
      "Epoch 90/50000\n",
      "44/44 [==============================] - 10s 222ms/step - loss: 0.0715\n",
      "Epoch 91/50000\n",
      "44/44 [==============================] - 10s 223ms/step - loss: 0.0738\n",
      "Epoch 92/50000\n",
      "44/44 [==============================] - 10s 218ms/step - loss: 0.0713\n",
      "Epoch 93/50000\n",
      "44/44 [==============================] - 10s 220ms/step - loss: 0.0707\n",
      "Epoch 94/50000\n",
      "44/44 [==============================] - 10s 222ms/step - loss: 0.0720\n",
      "Epoch 95/50000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44/44 [==============================] - 10s 216ms/step - loss: 0.0704\n",
      "Epoch 96/50000\n",
      "44/44 [==============================] - 10s 217ms/step - loss: 0.0704\n",
      "Epoch 97/50000\n",
      "44/44 [==============================] - 9s 215ms/step - loss: 0.0689\n",
      "Epoch 98/50000\n",
      "44/44 [==============================] - 10s 218ms/step - loss: 0.0673\n",
      "Epoch 99/50000\n",
      "44/44 [==============================] - 10s 216ms/step - loss: 0.0687\n",
      "Epoch 100/50000\n",
      "44/44 [==============================] - 10s 217ms/step - loss: 0.0680\n",
      "Epoch 101/50000\n",
      "44/44 [==============================] - 10s 221ms/step - loss: 0.0671\n",
      "Epoch 102/50000\n",
      "44/44 [==============================] - 10s 217ms/step - loss: 0.0668\n",
      "Epoch 103/50000\n",
      "44/44 [==============================] - 9s 216ms/step - loss: 0.0674\n",
      "Epoch 104/50000\n",
      "44/44 [==============================] - 10s 216ms/step - loss: 0.0665\n",
      "Epoch 105/50000\n",
      "44/44 [==============================] - 10s 230ms/step - loss: 0.0653\n",
      "Epoch 106/50000\n",
      "44/44 [==============================] - 10s 218ms/step - loss: 0.0646\n",
      "Epoch 107/50000\n",
      "44/44 [==============================] - 10s 233ms/step - loss: 0.0658\n",
      "Epoch 108/50000\n",
      "44/44 [==============================] - 10s 218ms/step - loss: 0.0645\n",
      "Epoch 109/50000\n",
      "44/44 [==============================] - 10s 217ms/step - loss: 0.0643\n",
      "Epoch 110/50000\n",
      "44/44 [==============================] - 10s 216ms/step - loss: 0.0629\n",
      "Epoch 111/50000\n",
      "44/44 [==============================] - 10s 222ms/step - loss: 0.0635\n",
      "Epoch 112/50000\n",
      "44/44 [==============================] - 10s 217ms/step - loss: 0.0630\n",
      "Epoch 113/50000\n",
      "44/44 [==============================] - 10s 220ms/step - loss: 0.0630\n",
      "Epoch 114/50000\n",
      "44/44 [==============================] - 10s 218ms/step - loss: 0.0614\n",
      "Epoch 115/50000\n",
      "44/44 [==============================] - 10s 218ms/step - loss: 0.0608\n",
      "Epoch 116/50000\n",
      "44/44 [==============================] - 10s 218ms/step - loss: 0.0607\n",
      "Epoch 117/50000\n",
      "44/44 [==============================] - 9s 216ms/step - loss: 0.0605\n",
      "Epoch 118/50000\n",
      "44/44 [==============================] - 10s 219ms/step - loss: 0.0596\n",
      "Epoch 119/50000\n",
      "44/44 [==============================] - 10s 221ms/step - loss: 0.0599\n",
      "Epoch 120/50000\n",
      "44/44 [==============================] - 10s 218ms/step - loss: 0.0598\n",
      "Epoch 121/50000\n",
      "44/44 [==============================] - 10s 219ms/step - loss: 0.0602\n",
      "Epoch 122/50000\n",
      "44/44 [==============================] - 10s 225ms/step - loss: 0.0579\n",
      "Epoch 123/50000\n",
      "44/44 [==============================] - 10s 221ms/step - loss: 0.0577\n",
      "Epoch 124/50000\n",
      "44/44 [==============================] - 10s 218ms/step - loss: 0.0570\n",
      "Epoch 125/50000\n",
      "44/44 [==============================] - 10s 218ms/step - loss: 0.0570\n",
      "Epoch 126/50000\n",
      "44/44 [==============================] - 10s 220ms/step - loss: 0.0568\n",
      "Epoch 127/50000\n",
      "44/44 [==============================] - 11s 250ms/step - loss: 0.0571\n",
      "Epoch 128/50000\n",
      "44/44 [==============================] - 10s 230ms/step - loss: 0.0565\n",
      "Epoch 129/50000\n",
      "44/44 [==============================] - 10s 228ms/step - loss: 0.0560\n",
      "Epoch 130/50000\n",
      "44/44 [==============================] - 10s 223ms/step - loss: 0.0557\n",
      "Epoch 131/50000\n",
      "44/44 [==============================] - 10s 224ms/step - loss: 0.0550\n",
      "Epoch 132/50000\n",
      "44/44 [==============================] - 10s 222ms/step - loss: 0.0533\n",
      "Epoch 133/50000\n",
      "44/44 [==============================] - 10s 221ms/step - loss: 0.0543\n",
      "Epoch 134/50000\n",
      "44/44 [==============================] - 10s 219ms/step - loss: 0.0540\n",
      "Epoch 135/50000\n",
      "44/44 [==============================] - 10s 230ms/step - loss: 0.0530\n",
      "Epoch 136/50000\n",
      "44/44 [==============================] - 10s 223ms/step - loss: 0.0532\n",
      "Epoch 137/50000\n",
      "44/44 [==============================] - 10s 223ms/step - loss: 0.0527\n",
      "Epoch 138/50000\n",
      "44/44 [==============================] - 10s 223ms/step - loss: 0.0532\n",
      "Epoch 139/50000\n",
      "44/44 [==============================] - 10s 226ms/step - loss: 0.0521\n",
      "Epoch 140/50000\n",
      "44/44 [==============================] - 10s 220ms/step - loss: 0.0522\n",
      "Epoch 141/50000\n",
      "44/44 [==============================] - 10s 220ms/step - loss: 0.0516\n",
      "Epoch 142/50000\n",
      "44/44 [==============================] - 10s 221ms/step - loss: 0.0510\n",
      "Epoch 143/50000\n",
      "44/44 [==============================] - 10s 221ms/step - loss: 0.0503\n",
      "Epoch 144/50000\n",
      "44/44 [==============================] - 10s 224ms/step - loss: 0.0505\n",
      "Epoch 145/50000\n",
      "44/44 [==============================] - 10s 221ms/step - loss: 0.0513\n",
      "Epoch 146/50000\n",
      "44/44 [==============================] - 10s 221ms/step - loss: 0.0505\n",
      "Epoch 147/50000\n",
      "44/44 [==============================] - 10s 221ms/step - loss: 0.0499\n",
      "Epoch 148/50000\n",
      "44/44 [==============================] - 10s 223ms/step - loss: 0.0498\n",
      "Epoch 149/50000\n",
      "44/44 [==============================] - 10s 225ms/step - loss: 0.0485\n",
      "Epoch 150/50000\n",
      "44/44 [==============================] - 10s 225ms/step - loss: 0.0487\n",
      "Epoch 151/50000\n",
      "44/44 [==============================] - 10s 224ms/step - loss: 0.0488\n",
      "Epoch 152/50000\n",
      "44/44 [==============================] - 10s 224ms/step - loss: 0.0484\n",
      "Epoch 153/50000\n",
      "44/44 [==============================] - 10s 227ms/step - loss: 0.0475\n",
      "Epoch 154/50000\n",
      "44/44 [==============================] - 10s 222ms/step - loss: 0.0471\n",
      "Epoch 155/50000\n",
      "44/44 [==============================] - 10s 222ms/step - loss: 0.0469\n",
      "Epoch 156/50000\n",
      "44/44 [==============================] - 10s 223ms/step - loss: 0.0472\n",
      "Epoch 157/50000\n",
      "44/44 [==============================] - 10s 222ms/step - loss: 0.0468\n",
      "Epoch 158/50000\n",
      "44/44 [==============================] - 10s 222ms/step - loss: 0.0465\n",
      "Epoch 159/50000\n",
      "44/44 [==============================] - 10s 221ms/step - loss: 0.0462\n",
      "Epoch 160/50000\n",
      "44/44 [==============================] - 10s 226ms/step - loss: 0.0462\n",
      "Epoch 161/50000\n",
      "44/44 [==============================] - 10s 223ms/step - loss: 0.0459\n",
      "Epoch 162/50000\n",
      "44/44 [==============================] - 10s 226ms/step - loss: 0.0452\n",
      "Epoch 163/50000\n",
      "44/44 [==============================] - 10s 226ms/step - loss: 0.0453\n",
      "Epoch 164/50000\n",
      "44/44 [==============================] - 10s 220ms/step - loss: 0.0447\n",
      "Epoch 165/50000\n",
      "44/44 [==============================] - 10s 221ms/step - loss: 0.0444\n",
      "Epoch 166/50000\n",
      "44/44 [==============================] - 10s 221ms/step - loss: 0.0446\n",
      "Epoch 167/50000\n",
      "44/44 [==============================] - 10s 223ms/step - loss: 0.0439\n",
      "Epoch 168/50000\n",
      "44/44 [==============================] - 10s 223ms/step - loss: 0.0440\n",
      "Epoch 169/50000\n",
      "44/44 [==============================] - 10s 224ms/step - loss: 0.0440\n",
      "Epoch 170/50000\n",
      "44/44 [==============================] - 10s 223ms/step - loss: 0.0433\n",
      "Epoch 171/50000\n",
      "44/44 [==============================] - 10s 223ms/step - loss: 0.0423\n",
      "Epoch 172/50000\n",
      "44/44 [==============================] - 10s 226ms/step - loss: 0.0430\n",
      "Epoch 173/50000\n",
      "44/44 [==============================] - 10s 223ms/step - loss: 0.0423\n",
      "Epoch 174/50000\n",
      "44/44 [==============================] - 10s 223ms/step - loss: 0.0426\n",
      "Epoch 175/50000\n",
      "44/44 [==============================] - 10s 225ms/step - loss: 0.0426\n",
      "Epoch 176/50000\n",
      "44/44 [==============================] - 10s 221ms/step - loss: 0.0421\n",
      "Epoch 177/50000\n",
      "44/44 [==============================] - 10s 221ms/step - loss: 0.0418\n",
      "Epoch 178/50000\n",
      "44/44 [==============================] - 10s 220ms/step - loss: 0.0416\n",
      "Epoch 179/50000\n",
      "44/44 [==============================] - 10s 222ms/step - loss: 0.0419\n",
      "Epoch 180/50000\n",
      "44/44 [==============================] - 10s 222ms/step - loss: 0.0411\n",
      "Epoch 181/50000\n",
      "44/44 [==============================] - 10s 225ms/step - loss: 0.0412\n",
      "Epoch 182/50000\n",
      "44/44 [==============================] - 10s 223ms/step - loss: 0.0403\n",
      "Epoch 183/50000\n",
      "44/44 [==============================] - 10s 233ms/step - loss: 0.0406\n",
      "Epoch 184/50000\n",
      "44/44 [==============================] - 11s 239ms/step - loss: 0.0401\n",
      "Epoch 185/50000\n",
      "44/44 [==============================] - 10s 226ms/step - loss: 0.0405\n",
      "Epoch 186/50000\n",
      "44/44 [==============================] - 10s 222ms/step - loss: 0.0399\n",
      "Epoch 187/50000\n",
      "44/44 [==============================] - 10s 225ms/step - loss: 0.0394\n",
      "Epoch 188/50000\n",
      "44/44 [==============================] - 10s 223ms/step - loss: 0.0396\n",
      "Epoch 189/50000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44/44 [==============================] - 10s 219ms/step - loss: 0.0392\n",
      "Epoch 190/50000\n",
      "44/44 [==============================] - 10s 219ms/step - loss: 0.0391\n",
      "Epoch 191/50000\n",
      "44/44 [==============================] - 10s 220ms/step - loss: 0.0393\n",
      "Epoch 192/50000\n",
      "44/44 [==============================] - 10s 227ms/step - loss: 0.0387\n",
      "Epoch 193/50000\n",
      "44/44 [==============================] - 10s 221ms/step - loss: 0.0387\n",
      "Epoch 194/50000\n",
      "44/44 [==============================] - 10s 223ms/step - loss: 0.0385\n",
      "Epoch 195/50000\n",
      "44/44 [==============================] - 10s 220ms/step - loss: 0.0384\n",
      "Epoch 196/50000\n",
      "44/44 [==============================] - 10s 224ms/step - loss: 0.0381\n",
      "Epoch 197/50000\n",
      "44/44 [==============================] - 10s 221ms/step - loss: 0.0375\n",
      "Epoch 198/50000\n",
      "44/44 [==============================] - 10s 219ms/step - loss: 0.0377\n",
      "Epoch 199/50000\n",
      "44/44 [==============================] - 10s 223ms/step - loss: 0.0371\n",
      "Epoch 200/50000\n",
      "44/44 [==============================] - 10s 226ms/step - loss: 0.0369\n",
      "Epoch 201/50000\n",
      "44/44 [==============================] - 10s 220ms/step - loss: 0.0365\n",
      "Epoch 202/50000\n",
      "44/44 [==============================] - 10s 220ms/step - loss: 0.0368\n",
      "Epoch 203/50000\n",
      "44/44 [==============================] - 10s 221ms/step - loss: 0.0364\n",
      "Epoch 204/50000\n",
      "44/44 [==============================] - 10s 222ms/step - loss: 0.0362\n",
      "Epoch 205/50000\n",
      "44/44 [==============================] - 10s 221ms/step - loss: 0.0367\n",
      "Epoch 206/50000\n",
      "44/44 [==============================] - 10s 223ms/step - loss: 0.0358\n",
      "Epoch 207/50000\n",
      "44/44 [==============================] - 10s 221ms/step - loss: 0.0362\n",
      "Epoch 208/50000\n",
      "44/44 [==============================] - 10s 221ms/step - loss: 0.0353\n",
      "Epoch 209/50000\n",
      "44/44 [==============================] - 10s 224ms/step - loss: 0.0356\n",
      "Epoch 210/50000\n",
      "44/44 [==============================] - 10s 221ms/step - loss: 0.0353\n",
      "Epoch 211/50000\n",
      "44/44 [==============================] - 10s 221ms/step - loss: 0.0355\n",
      "Epoch 212/50000\n",
      "44/44 [==============================] - 10s 224ms/step - loss: 0.0350\n",
      "Epoch 213/50000\n",
      "44/44 [==============================] - 10s 221ms/step - loss: 0.0349\n",
      "Epoch 214/50000\n",
      "44/44 [==============================] - 10s 224ms/step - loss: 0.0348\n",
      "Epoch 215/50000\n",
      "44/44 [==============================] - 10s 222ms/step - loss: 0.0346\n",
      "Epoch 216/50000\n",
      "44/44 [==============================] - 10s 222ms/step - loss: 0.0343\n",
      "Epoch 217/50000\n",
      "44/44 [==============================] - 10s 221ms/step - loss: 0.0342\n",
      "Epoch 218/50000\n",
      "44/44 [==============================] - 10s 222ms/step - loss: 0.0342\n",
      "Epoch 219/50000\n",
      "44/44 [==============================] - 10s 223ms/step - loss: 0.0337\n",
      "Epoch 220/50000\n",
      "44/44 [==============================] - 10s 220ms/step - loss: 0.0338\n",
      "Epoch 221/50000\n",
      "44/44 [==============================] - 10s 224ms/step - loss: 0.0334\n",
      "Epoch 222/50000\n",
      "44/44 [==============================] - 10s 221ms/step - loss: 0.0333\n",
      "Epoch 223/50000\n",
      "44/44 [==============================] - 10s 225ms/step - loss: 0.0330\n",
      "Epoch 224/50000\n",
      "44/44 [==============================] - 10s 221ms/step - loss: 0.0334\n",
      "Epoch 225/50000\n",
      "44/44 [==============================] - 10s 225ms/step - loss: 0.0330\n",
      "Epoch 226/50000\n",
      "44/44 [==============================] - 10s 221ms/step - loss: 0.0330\n",
      "Epoch 227/50000\n",
      "44/44 [==============================] - 10s 221ms/step - loss: 0.0328\n",
      "Epoch 228/50000\n",
      "44/44 [==============================] - 10s 222ms/step - loss: 0.0326\n",
      "Epoch 229/50000\n",
      "44/44 [==============================] - 10s 224ms/step - loss: 0.0324\n",
      "Epoch 230/50000\n",
      "44/44 [==============================] - 10s 221ms/step - loss: 0.0328\n",
      "Epoch 231/50000\n",
      "44/44 [==============================] - 10s 226ms/step - loss: 0.0322\n",
      "Epoch 232/50000\n",
      "44/44 [==============================] - 10s 222ms/step - loss: 0.0323\n",
      "Epoch 233/50000\n",
      "23/44 [==============>...............] - ETA: 4s - loss: 0.0302"
     ]
    }
   ],
   "source": [
    "autoencoder.fit_generator(datagen.flow(X, Y, batch_size=1),\n",
    "                    steps_per_epoch=len(X), epochs=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#autoencoder.fit((train_generator, train_generator),\n",
    "                #steps_per_epoch=44,\n",
    "                #epochs=500000)\n",
    "                #batch_size=128,\n",
    "                #shuffle=False)\n",
    "                #validation_data=(x_test, x_test),\n",
    "                #callbacks=[TensorBoard(log_dir='/tmp/autoencoder')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 779,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.save('autoencoder_loss_10.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=autoencoder.predict(X[1].reshape(-1,28,28,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 787,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 5760x1152 with 43 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "decoded_imgs = autoencoder.predict(X)\n",
    "\n",
    "n = 43\n",
    "plt.figure(figsize=(80, 16))\n",
    "for i in range(n):\n",
    "    # display original\n",
    "    ax = plt.subplot(2, n+1, i+1)\n",
    "    #ax = plt.subplot(2, n, i + n)\n",
    "    plt.imshow(np.array(X[i]).reshape(80, 80,3))\n",
    "    #plt.imshow(X[i])\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    ax = plt.subplot(2, n+1, i+1)\n",
    "    # display reconstruction\n",
    "    #ax = plt.subplot(2, n, i + n)\n",
    "    plt.imshow(np.array(decoded_imgs[i]).reshape(80, 80,3))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_to_img(autoencoder.predict(np.array(X[1].reshape(1,28,28,1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=load_model(r\"C:\\Users\\dedunn\\Desktop\\weights-val_74-0.98.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 224, 224, 32)      2432      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 224, 224, 32)      0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 224, 224, 32)      128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 221, 221, 32)      0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 221, 221, 32)      0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 221, 221, 32)      128       \n",
      "_________________________________________________________________\n",
      "average_pooling2d (AveragePo (None, 110, 110, 32)      0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 387200)            0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 43)                16649643  \n",
      "=================================================================\n",
      "Total params: 16,652,331\n",
      "Trainable params: 16,652,203\n",
      "Non-trainable params: 128\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "model.add(tensorflow.keras.layers.Conv2D(input_shape=(224,224,3),filters=32, kernel_size=(5,5),activation=\"relu\", padding=\"same\")),\n",
    "model.add(tensorflow.keras.layers.Dropout(0.5)),\n",
    "model.add(tensorflow.keras.layers.BatchNormalization(axis=-1)),\n",
    "#model.add(tensorflow.keras.layers.Activation(\"relu\")),\n",
    "model.add(tensorflow.keras.layers.MaxPooling2D(pool_size=(5, 5), strides=(1,1), padding='valid')),\n",
    "#model.add(tensorflow.keras.layers.Conv2D(filters=64, kernel_size=(3,3), padding=\"same\",activation=\"relu\")),\n",
    "#model.add(tensorflow.keras.layers.Conv2D(filters=64, kernel_size=(3,3), padding=\"same\",activation=\"relu\")),\n",
    "#model.add(tensorflow.keras.layers.Dropout(0.5)),\n",
    "#model.add(tensorflow.keras.layers.BatchNormalization(axis=-1)),\n",
    "#model.add(tensorflow.keras.layers.Activation(\"relu\")),\n",
    "#model.add(tensorflow.keras.layers.MaxPooling2D(pool_size=(4, 4), strides=(1,1), padding='valid')),\n",
    "#model.add(tensorflow.keras.layers.Dropout(0.5)),\n",
    "#model.add(tensorflow.keras.layers.Conv2D(filters=128, kernel_size=(3,3), padding=\"same\",activation=\"relu\")),\n",
    "model.add(tensorflow.keras.layers.BatchNormalization(axis=-1)),\n",
    "#model.add(tensorflow.keras.layers.Activation(\"relu\")),\n",
    "#model.add(tensorflow.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2,2), padding='valid')),\n",
    "#model.add(tensorflow.keras.layers.AveragePooling2D(pool_size=(4, 4), strides=(1,1), padding='valid')),\n",
    "#model.add(tensorflow.keras.layers.Dropout(0.5)),\n",
    "#model.add(tensorflow.keras.layers.Conv2D(filters=224, kernel_size=(3,3), padding=\"same\", activation=\"relu\")),\n",
    "#model.add(tensorflow.keras.layers.Dropout(0.5)),\n",
    "#model.add(tensorflow.keras.layers.BatchNormalization(axis=-1)),\n",
    "#model.add(tensorflow.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(1,1), padding='valid')),\n",
    "model.add(tensorflow.keras.layers.AveragePooling2D(pool_size=(2, 2), strides=None, padding='valid', data_format=None)),\n",
    "model.add(tensorflow.keras.layers.Flatten()),\n",
    "model.add(tensorflow.keras.layers.Dense(44, activation='softmax')),"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
