{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# import the necessary packages\n",
    "from keras.models import Sequential\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D \n",
    "from keras.layers.core import Activation\n",
    "from keras.layers.core import Flatten\n",
    "from keras.layers.core import Dropout\n",
    "from keras.layers.core import Dense\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of creating a CNN model with many VGG blocks\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.utils import plot_model\n",
    "from keras.utils import to_categorical\n",
    "# function for creating a vgg block\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.applications.vgg16 import decode_predictions\n",
    "from keras.applications.vgg16 import VGG16\n",
    "# load an image from file\n",
    "import sys\n",
    "from numpy import load\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.image import imread\n",
    "from numpy import expand_dims\n",
    "\n",
    "import pickle\n",
    "from keras.preprocessing.image import array_to_img\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from numpy import expand_dims\n",
    "import numpy as np\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=load_img('/Users/denisdunn/Desktop/ten_pictures/eye_shadow_contour_brush_traditional_white_5112x3616.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = expand_dims(data, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "    featurewise_center=True,\n",
    "    featurewise_std_normalization=True,\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.3,\n",
    "    height_shift_range=0.3,\n",
    "    brightness_range=(.4,1.6),\n",
    "    shear_range=0.3,\n",
    "    zoom_range=0.3,\n",
    "    horizontal_flip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "it = datagen.flow(samples,batch_size=1,save_to_dir='/Users/denisdunn/Desktop/test_images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/keras_preprocessing/image/image_data_generator.py:699: UserWarning: This ImageDataGenerator specifies `featurewise_center`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
      "  warnings.warn('This ImageDataGenerator specifies '\n",
      "/anaconda3/lib/python3.7/site-packages/keras_preprocessing/image/image_data_generator.py:707: UserWarning: This ImageDataGenerator specifies `featurewise_std_normalization`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
      "  warnings.warn('This ImageDataGenerator specifies '\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# generate samples and plot\n",
    "for i in range(5):\n",
    "  # define subplot\n",
    "    #pyplot.subplot(330 + 1 + i)\n",
    "  # generate batch of images\n",
    "    batch = it.next()\n",
    "# convert to unsigned integers for viewing \n",
    "    #image = batch[0].astype('uint32')\n",
    "    \n",
    "# plot raw pixel data \n",
    "    #pyplot.imshow(image)\n",
    "# show the figure\n",
    "#pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'image' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-07958332d68e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0marray_to_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'image' is not defined"
     ]
    }
   ],
   "source": [
    "array_to_img(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_processing(test):\n",
    "    #array_test=img_to_array(test)\n",
    "    array_resize=cv2.resize(test,(IMG_SIZE,IMG_SIZE))\n",
    "    return (np.array(array_resize).reshape(1,IMG_SIZE,IMG_SIZE,1))/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATADIR='/Users/denisdunn/Desktop/SmallDataDir'\n",
    "CATEGORIES=['woman','tuxedo_cologne','rn_radience_serum_cushion','oil_swatch','no_brush_sun_kissed','mac_studio_fix','F_19_aerin_perfume','eye_shadow_brush','Elevated_skin_love','clinque_for_men']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_data():\n",
    "    for category in CATEGORIES:\n",
    "        path =  os.path.join(DATADIR,category)\n",
    "        class_num = CATEGORIES.index(category)\n",
    "        for img in os.listdir(path):\n",
    "            try:\n",
    "                img_array = cv2.imread(os.path.join(path,img),cv2.IMREAD_GRAYSCALE)\n",
    "                \n",
    "                new_array = cv2.resize(img_array,(IMG_SIZE,IMG_SIZE))\n",
    "                test_image = new_array[...,::-1]\n",
    "                training_data.append([test_image,class_num])\n",
    "            except Exception as e:\n",
    "                pass\n",
    "create_training_data()\n",
    "#cv2.IMREAD_GRAYSCALE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n"
     ]
    }
   ],
   "source": [
    "print(len(training_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.shuffle(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "for features, label in training_data:   \n",
    "    X.append(features)\n",
    "    y.append(label)\n",
    "X=np.array(X).reshape(-1,IMG_SIZE,IMG_SIZE,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "y = to_categorical(y,num_classes=10,dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=X/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p=cv2.imread('/Users/denisdunn/Desktop/shirt_test.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = load_img('/Users/denisdunn/Desktop/Screen Shot 2019-12-10 at 1.50.12 PM.png', target_size=(224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = img_to_array(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = preprocess_input(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a new model with random weights and 10 classes\n",
    "new_input = Input(shape=(224, 224, 3))\n",
    "model = VGG16( weights='imagenet',input_tensor=new_input, classes=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out=model.predict(image)\n",
    "decode_predictions(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#model.compile(loss='categorical_crossentropy',optimizer = 'adam',metrics=['accuracy'])\n",
    "#model.fit(X,y,epochs=10,validation_split=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "300/300 [==============================] - 1s 2ms/step - loss: 2.4536 - accuracy: 0.1200\n",
      "Epoch 2/60\n",
      "300/300 [==============================] - 1s 2ms/step - loss: 2.2525 - accuracy: 0.1667\n",
      "Epoch 3/60\n",
      "300/300 [==============================] - 1s 2ms/step - loss: 2.0282 - accuracy: 0.2233\n",
      "Epoch 4/60\n",
      "300/300 [==============================] - 1s 2ms/step - loss: 1.6888 - accuracy: 0.3567\n",
      "Epoch 5/60\n",
      "300/300 [==============================] - 1s 2ms/step - loss: 1.4499 - accuracy: 0.4167\n",
      "Epoch 6/60\n",
      "300/300 [==============================] - 1s 2ms/step - loss: 1.3115 - accuracy: 0.4800\n",
      "Epoch 7/60\n",
      "300/300 [==============================] - 1s 2ms/step - loss: 1.1520 - accuracy: 0.5167\n",
      "Epoch 8/60\n",
      "300/300 [==============================] - 1s 2ms/step - loss: 1.0873 - accuracy: 0.5400\n",
      "Epoch 9/60\n",
      "300/300 [==============================] - 1s 2ms/step - loss: 1.0088 - accuracy: 0.6133\n",
      "Epoch 10/60\n",
      "300/300 [==============================] - 1s 2ms/step - loss: 0.9619 - accuracy: 0.5900\n",
      "Epoch 11/60\n",
      "300/300 [==============================] - 1s 2ms/step - loss: 0.9447 - accuracy: 0.6067\n",
      "Epoch 12/60\n",
      "300/300 [==============================] - 1s 2ms/step - loss: 0.9022 - accuracy: 0.5900\n",
      "Epoch 13/60\n",
      "300/300 [==============================] - 1s 2ms/step - loss: 0.8791 - accuracy: 0.6167\n",
      "Epoch 14/60\n",
      "300/300 [==============================] - 1s 2ms/step - loss: 0.8531 - accuracy: 0.6333\n",
      "Epoch 15/60\n",
      "300/300 [==============================] - 1s 2ms/step - loss: 0.7573 - accuracy: 0.6633\n",
      "Epoch 16/60\n",
      "300/300 [==============================] - 1s 2ms/step - loss: 0.7993 - accuracy: 0.6433\n",
      "Epoch 17/60\n",
      "300/300 [==============================] - 1s 2ms/step - loss: 0.8250 - accuracy: 0.6633\n",
      "Epoch 18/60\n",
      "300/300 [==============================] - 1s 2ms/step - loss: 0.7585 - accuracy: 0.6467\n",
      "Epoch 19/60\n",
      "300/300 [==============================] - 1s 2ms/step - loss: 0.7263 - accuracy: 0.6733\n",
      "Epoch 20/60\n",
      "300/300 [==============================] - 1s 2ms/step - loss: 0.6931 - accuracy: 0.7100\n",
      "Epoch 21/60\n",
      "300/300 [==============================] - 1s 2ms/step - loss: 0.7519 - accuracy: 0.6767\n",
      "Epoch 22/60\n",
      "300/300 [==============================] - 1s 2ms/step - loss: 0.8583 - accuracy: 0.6367\n",
      "Epoch 23/60\n",
      "300/300 [==============================] - 1s 2ms/step - loss: 0.6676 - accuracy: 0.7300\n",
      "Epoch 24/60\n",
      "300/300 [==============================] - 1s 2ms/step - loss: 0.6355 - accuracy: 0.7133\n",
      "Epoch 25/60\n",
      "300/300 [==============================] - 1s 2ms/step - loss: 0.6611 - accuracy: 0.7367\n",
      "Epoch 26/60\n",
      "300/300 [==============================] - 1s 2ms/step - loss: 0.6276 - accuracy: 0.7167\n",
      "Epoch 27/60\n",
      "300/300 [==============================] - 1s 2ms/step - loss: 0.6414 - accuracy: 0.7167\n",
      "Epoch 28/60\n",
      "300/300 [==============================] - 1s 2ms/step - loss: 0.6430 - accuracy: 0.6967\n",
      "Epoch 29/60\n",
      "300/300 [==============================] - 1s 2ms/step - loss: 0.6396 - accuracy: 0.7267\n",
      "Epoch 30/60\n",
      "300/300 [==============================] - 1s 2ms/step - loss: 0.6793 - accuracy: 0.7033\n",
      "Epoch 31/60\n",
      "300/300 [==============================] - 1s 2ms/step - loss: 0.6195 - accuracy: 0.7233\n",
      "Epoch 32/60\n",
      "300/300 [==============================] - 1s 2ms/step - loss: 0.5534 - accuracy: 0.7500\n",
      "Epoch 33/60\n",
      "300/300 [==============================] - 1s 2ms/step - loss: 0.5698 - accuracy: 0.7533\n",
      "Epoch 34/60\n",
      "300/300 [==============================] - 1s 2ms/step - loss: 0.6045 - accuracy: 0.7267\n",
      "Epoch 35/60\n",
      "300/300 [==============================] - 1s 2ms/step - loss: 0.6113 - accuracy: 0.7433\n",
      "Epoch 36/60\n",
      "300/300 [==============================] - 1s 2ms/step - loss: 0.6026 - accuracy: 0.7067\n",
      "Epoch 37/60\n",
      "300/300 [==============================] - 1s 2ms/step - loss: 0.6270 - accuracy: 0.7200\n",
      "Epoch 38/60\n",
      "300/300 [==============================] - 1s 2ms/step - loss: 0.5166 - accuracy: 0.7567\n",
      "Epoch 39/60\n",
      "300/300 [==============================] - 1s 2ms/step - loss: 0.5328 - accuracy: 0.7600\n",
      "Epoch 40/60\n",
      "300/300 [==============================] - 1s 2ms/step - loss: 0.6407 - accuracy: 0.7233\n",
      "Epoch 41/60\n",
      "300/300 [==============================] - 1s 2ms/step - loss: 0.5604 - accuracy: 0.7367\n",
      "Epoch 42/60\n",
      "300/300 [==============================] - 1s 2ms/step - loss: 0.5553 - accuracy: 0.7533\n",
      "Epoch 43/60\n",
      "300/300 [==============================] - 1s 2ms/step - loss: 0.5560 - accuracy: 0.7400\n",
      "Epoch 44/60\n",
      "300/300 [==============================] - 1s 2ms/step - loss: 0.5885 - accuracy: 0.7467\n",
      "Epoch 45/60\n",
      "300/300 [==============================] - 1s 2ms/step - loss: 0.7214 - accuracy: 0.7133\n",
      "Epoch 46/60\n",
      "300/300 [==============================] - 1s 2ms/step - loss: 0.5300 - accuracy: 0.7600\n",
      "Epoch 47/60\n",
      "300/300 [==============================] - 1s 2ms/step - loss: 0.5271 - accuracy: 0.7833\n",
      "Epoch 48/60\n",
      "300/300 [==============================] - 1s 2ms/step - loss: 0.4899 - accuracy: 0.7567\n",
      "Epoch 49/60\n",
      "300/300 [==============================] - 1s 2ms/step - loss: 0.4621 - accuracy: 0.8100\n",
      "Epoch 50/60\n",
      "300/300 [==============================] - 1s 2ms/step - loss: 0.5095 - accuracy: 0.7833\n",
      "Epoch 51/60\n",
      "300/300 [==============================] - 1s 2ms/step - loss: 0.5509 - accuracy: 0.7467\n",
      "Epoch 52/60\n",
      "300/300 [==============================] - 1s 2ms/step - loss: 0.5338 - accuracy: 0.7533\n",
      "Epoch 53/60\n",
      "300/300 [==============================] - 1s 2ms/step - loss: 0.4885 - accuracy: 0.7867\n",
      "Epoch 54/60\n",
      "300/300 [==============================] - 1s 2ms/step - loss: 0.4649 - accuracy: 0.7833\n",
      "Epoch 55/60\n",
      "300/300 [==============================] - 1s 2ms/step - loss: 0.5270 - accuracy: 0.7567\n",
      "Epoch 56/60\n",
      "300/300 [==============================] - 1s 2ms/step - loss: 0.4287 - accuracy: 0.8233\n",
      "Epoch 57/60\n",
      "300/300 [==============================] - 1s 2ms/step - loss: 0.4115 - accuracy: 0.8067\n",
      "Epoch 58/60\n",
      "300/300 [==============================] - 1s 2ms/step - loss: 0.5381 - accuracy: 0.7833\n",
      "Epoch 59/60\n",
      "300/300 [==============================] - 1s 2ms/step - loss: 0.4503 - accuracy: 0.7967\n",
      "Epoch 60/60\n",
      "300/300 [==============================] - 1s 2ms/step - loss: 0.4161 - accuracy: 0.8100\n",
      "300/300 [==============================] - 0s 120us/step\n",
      "Test accuracy: 0.8666666746139526\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(50,50,1)),\n",
    "    keras.layers.Dense(64, activation='relu'),\n",
    "\tkeras.layers.Dense(32, activation='relu'),\n",
    "    keras.layers.Dense(10, activation='softmax'),\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X, y, epochs=60, batch_size=1)\n",
    "\n",
    "test_loss, test_acc = model.evaluate(X, y)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(first_try)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "denis=cv2.imread('/Users/denisdunn/Desktop/test_images/_0_294.png',cv2.IMREAD_GRAYSCALE)\n",
    "#cv2.IMREAD_GRAYSCALE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer=image_processing(denis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clinque_for_men\n"
     ]
    }
   ],
   "source": [
    "result=model.predict_classes(answer)\n",
    "print(CATEGORIES[int(result)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.69564368e-05, 1.62700975e-09, 3.55995572e-25, 3.75705614e-15,\n",
       "        8.81901797e-05, 7.75962209e-19, 1.48114784e-11, 1.34603895e-14,\n",
       "        1.02063849e-15, 9.99874830e-01]], dtype=float32)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('/Users/denisdunn/Desktop/DATADIR/X',X)\n",
    "np.save('/Users/denisdunn/Desktop/DATADIR/y',y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "keras.layers.Conv2D(input_shape=(50,50,1),filters=64,kernel_size=(3,3),padding=\"same\", activation=\"relu\"),\n",
    "keras.layers.Conv2D(filters=64,kernel_size=(3,3),padding=\"same\", activation=\"relu\"),\n",
    "keras.layers.MaxPool2D(pool_size=(2,2),strides=(2,2)),\n",
    "keras.layers.Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"),\n",
    "keras.layers.Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"),\n",
    "keras.layers.MaxPool2D(pool_size=(2,2),strides=(2,2)),\n",
    "keras.layers.Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"),\n",
    "keras.layers.Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"),\n",
    "keras.layers.Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"),\n",
    "keras.layers.MaxPool2D(pool_size=(2,2),strides=(2,2)),\n",
    "keras.layers.Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"),\n",
    "keras.layers.Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"),\n",
    "keras.layers.Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"),\n",
    "keras.layers.MaxPool2D(pool_size=(2,2),strides=(2,2)),\n",
    "keras.layers.Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"),\n",
    "keras.layers.Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"),\n",
    "keras.layers.Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"),\n",
    "keras.layers.MaxPool2D(pool_size=(2,2),strides=(2,2)),\n",
    "keras.layers.Flatten(),\n",
    "keras.layers.Dense(units=4096,activation=\"relu\"),\n",
    "keras.layers.Dense(units=4096,activation=\"relu\"),\n",
    "keras.layers.Dense(units=3, activation=\"softmax\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X, y, epochs=10, batch_size=1)\n",
    "\n",
    "test_loss, test_acc = model.evaluate(X, y)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0.0\n"
     ]
    }
   ],
   "source": [
    "import cv2,time\n",
    "import numpy as np\n",
    "print(cv2.__version__)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-5-2c9c3ff08185>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-5-2c9c3ff08185>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    anaconda3/lib/python3.7/site-packages/cv2/data/haarcascade_frontalface_default.xml()\u001b[0m\n\u001b[0m                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "/anaconda3/lib/python3.7/site-packages/cv2/data/haarcascade_frontalface_default.xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_cascade=cv2.CascadeClassifier('/anaconda3/lib/python3.7/site-packages/cv2/data/haarcascade_frontalface_default.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gray=cv2.imread('/Users/denisdunn/Desktop/Ivb19_0881-319x319.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "faces=face_cascade.detectMultiScale(gray,scaleFactor=1.05,minNeighbors=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[100  29  31  31]]\n"
     ]
    }
   ],
   "source": [
    "print(faces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x,y,w,h, in faces:\n",
    "    img = cv2.rectangle(gray, (x,y), (x+w,y+h),(0,255,0),3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow('gray',img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import cv2\n",
    "\n",
    "#cvim2disp = cv2.imread('/Users/denisdunn/Desktop/Ivb19_0881-319x319.png')\n",
    "cv2.imshow('img', img)\n",
    "while(True):\n",
    "    k = cv2.waitKey(33)\n",
    "    if k == -1:  # if no key was pressed, -1 is returned\n",
    "        continue\n",
    "    else:\n",
    "        break\n",
    "cv2.destroyWindow('img')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video=cv2.VideoCapture(0)\n",
    "check,frame=video.read()\n",
    "time.sleep(3) \n",
    "cv2.imshow('Capture',frame)\n",
    "cv2.waitKey(0)\n",
    "video.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/denisdunn/Desktop/denisdunn'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.destroyWindow('gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '/Users/denisdunn/Desktop/shirt_test.png'\n",
    "img = cv2.imread(filename)\n",
    "gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "gray = np.float32(gray)\n",
    "dst = cv2.cornerHarris(gray,2,3,0.04)\n",
    "\n",
    "#result is dilated for marking the corners, not important\n",
    "dst = cv2.dilate(dst,None)\n",
    "\n",
    "# Threshold for an optimal value, it may vary depending on the image.\n",
    "img[dst>0.01*dst.max()]=[0,0,255]\n",
    "\n",
    "cv2.imshow('dst',img)\n",
    "while(True):\n",
    "    k = cv2.waitKey(33)\n",
    "    if k == -1:  # if no key was pressed, -1 is returned\n",
    "        continue\n",
    "    else:\n",
    "        break\n",
    "cv2.destroyWindow('img')\n",
    "#if cv2.waitKey(0) & 0xff == 27:\n",
    "    #cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '/Users/denisdunn/Desktop/shirt_test.png'\n",
    "img = cv2.imread(filename)\n",
    "gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "gray = np.float32(gray)\n",
    "dst = cv2.cornerHarris(gray,2,3,0.04)\n",
    "\n",
    "#result is dilated for marking the corners, not important\n",
    "dst = cv2.dilate(dst,None)\n",
    "\n",
    "# Threshold for an optimal value, it may vary depending on the image.\n",
    "img[dst>0.01*dst.max()]=[0,0,255]\n",
    "\n",
    "cv2.imshow('dst',img)\n",
    "cv2.waitKey()\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(\"image.jpg\")\n",
    "cv2.startWindowThread()\n",
    "cv2.namedWindow(\"preview\")\n",
    "cv2.imshow(\"preview\", im"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
