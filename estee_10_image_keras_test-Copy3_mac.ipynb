{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# import the necessary packages\n",
    "from keras.models import Sequential\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D \n",
    "from keras.layers.core import Activation\n",
    "from keras.layers.core import Flatten\n",
    "from keras.layers.core import Dropout\n",
    "from keras.layers.core import Dense\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "\n",
    "Image.MAX_IMAGE_PIXELS = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.layers import Conv2D\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of creating a CNN model with many VGG blocks\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.utils import plot_model\n",
    "from keras.utils import to_categorical\n",
    "# function for creating a vgg block\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.applications.vgg16 import decode_predictions\n",
    "from keras.applications.vgg16 import VGG16\n",
    "# load an image from file\n",
    "import sys\n",
    "from numpy import load\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.image import imread\n",
    "from numpy import expand_dims\n",
    "\n",
    "import pickle\n",
    "from keras.preprocessing.image import array_to_img\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from numpy import expand_dims\n",
    "import numpy as np\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'tensorflow_core._api.v2.version' from '/Users/denisdunn/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/_api/v2/version/__init__.py'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/Users/denisdunn/Desktop/full_aerin/fifty_plus_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "directory=os.listdir(\"/Users/denisdunn/Desktop/full_aerin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "file=('/Users/denisdunn/Desktop/full_aerin/fifty_plus_class/{}').format(directory[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory.remove('.DS_Store')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "direct=('/Users/denisdunn/Desktop/full_aerin/{}').format(directory[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "#newPath = shutil.move(file, direct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.makedirs(direct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for files in directory:\n",
    "    direct=('/Users/denisdunn/Desktop/full_aerin/{}').format(files)\n",
    "    #os.makedirs(direct)\n",
    "    file=('/Users/denisdunn/Desktop/fourty_five_estee/{}').format(files)\n",
    "    shutil.move(file, direct)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=load_img(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "/Users/denisdunn/Desktop/full_aerin/AERIN_IM_MOODBOARD_LR.jpg/AERIN_IM_MOODBOARD_LR.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for picture in directory:\n",
    "    try:\n",
    "        pic=('/Users/denisdunn/Desktop/full_aerin/{}/{}').format(picture,picture)\n",
    "        data=load_img(pic)\n",
    "        samples = expand_dims(data, 0)\n",
    "        saved=('/Users/denisdunn/Desktop/full_aerin/{}').format(picture)\n",
    "        it = datagen.flow(samples,batch_size=1,save_to_dir=saved)\n",
    "        for i in range(10):\n",
    "            batch = it.next()\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "picture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=('/Users/denisdunn/Desktop/full_aerin/{}/{}').format('picture','picture')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=load_img('/Users/denisdunn/Desktop/full_aerin/F19_AERIN_LIM_DIGITAL_1_HORIZ.jpg/F19_AERIN_LIM_DIGITAL_1_HORIZ.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = expand_dims(data, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "    #featurewise_center=True,\n",
    "    #featurewise_std_normalization=True,\n",
    "    preprocessing_function=preprocess_input,\n",
    "    #rescale=1/255,\n",
    "    rotation_range=10,\n",
    "    width_shift_range=20,\n",
    "    height_shift_range=20,\n",
    "    brightness_range=(.7,1.2),\n",
    "    shear_range=0.3,\n",
    "    zoom_range=[.7,1.6],\n",
    "    channel_shift_range=20.0,\n",
    "    fill_mode=\"nearest\",)\n",
    "    #horizontal_flip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "it = datagen.flow(samples,batch_size=1,save_to_dir='/Users/denisdunn/Desktop/full_aerin/F19_AERIN_LIM_DIGITAL_1_HORIZ.jpg')\n",
    "#it = datagen.flow(samples,batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# generate samples and plot\n",
    "for i in range(20):\n",
    "  # define subplot\n",
    "    #plt.subplot(330 + 1 + i)\n",
    "  # generate batch of images\n",
    "    batch = it.next()\n",
    "# convert to unsigned integers for viewing \n",
    "    #image = batch[0].astype('uint32')\n",
    "    \n",
    "# plot raw pixel data \n",
    "    #plt.imshow(image)\n",
    "# show the figure\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "array_to_img(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "print(glob.glob(\"/Users/denisdunn/Desktop/full_aerin/*.jpg\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat=os.listdir(\"/Users/denisdunn/Desktop/full_aerin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat.remove('.DS_Store')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['F19_AERIN_LIM_DIGITAL_1_HORIZ.jpg',\n",
       " 'F19_AERIN_AM_DIGITAL_5_SQUARE.jpg',\n",
       " 'AERIN_IM_MOODBOARD_LR.jpg',\n",
       " 'F19_AERIN_HOL_DIGITAL_HORIZ_10.jpg',\n",
       " 'F19_AERIN_LIM_100ML_SILO_1.tif',\n",
       " '.DS_Store',\n",
       " 'F19_AERIN_HOL_DIGITAL_SQUARE_2_v2.jpg',\n",
       " '191001_LSF_SHOT23_5967_2480DI_F_ADOBERGB.jpg',\n",
       " 'S20_AERIN_WG_DIGITAL_SQUARE_09.jpg',\n",
       " 'HOL18_AERIN_HOLIDAY_MODEL.tif',\n",
       " 'AERIN_TV_MOODBOARD_LR.jpg',\n",
       " 'AERIN_MH_MOODBOARD_LR.jpg',\n",
       " 'FL17_AERIN_TLJ_TLS_MODEL_DPS_HR.tif',\n",
       " 'Aerin_Office_PhotoFrame_8x10.tif',\n",
       " 'SP19_Aerin_Aegea_Blossom_Model_Loose_REL_72dpi.jpg',\n",
       " 'AERIN_WS_MOODBOARD_LR.jpg',\n",
       " 'SP17_AERIN_DOR_HERO_1.jpg',\n",
       " 'F19_AERIN_LIM_MODEL_R300.tif',\n",
       " 'F19_AERIN_LIM_MODEL_F72.tif',\n",
       " 'F19_AERIN_AM_DIGITAL_2_SQUARE.jpg',\n",
       " 'F19_AERIN_HOL_DIGITAL_SQUARE_1_v2.jpg',\n",
       " 'F19_AERIN_LIM_BC_SILO.tif',\n",
       " 'AERIN_LP_MOODBOARD_LR.jpg',\n",
       " 'F19_AERIN_LIM_DIGITAL_4_VERT.jpg',\n",
       " 'F19_AERIN_LIM_DIGITAL_6.tif',\n",
       " 'AERIN_GR_MOODBOARD_LR.jpg',\n",
       " 'F19_AERIN_BCRF_SILO.tif',\n",
       " 'S20_AERIN_WG_DIGITAL_SQUARE_10.jpg',\n",
       " 'F19_AERIN_NOAM_SET.tif',\n",
       " 'S19_AERIN_AB_DIGITAL_5_F600.tif',\n",
       " 'S20_AERIN_WG_DIGITAL_SQUARE_11.jpg',\n",
       " 'FL17_AERIN_TLJ_TLS_HERO.jpg',\n",
       " 'F19_AERIN_HOL_DIGITAL_HORIZ_9.jpg',\n",
       " 'S17_AERIN_ROSE_COL_MODEL_DPS_72dpi.jpg',\n",
       " 'AERIN_AM_MOODBOARD_LR.jpg',\n",
       " 'F19_AERIN_AM_DIGITAL_3_SQUARE.jpg',\n",
       " 'AERIN_IJ_MOODBOARD_LR.jpg',\n",
       " 'S20_AERIN_WG_DIGITAL_SQUARE_02.jpg',\n",
       " 'F19_AERIN_AM_DIGITAL_4_SQUARE_v3.jpg',\n",
       " 'F19_AERIN_EDV_MODEL_EMEA_R72.tif',\n",
       " 'S20_AERIN_WG_DIGITAL_SQUARE_01.jpg',\n",
       " 'S20_AERIN_WG_DIGITAL_HZTL_13.jpg',\n",
       " 'S19_AERIN_AB_RB_SILO_1.jpg',\n",
       " 'S20_AERIN_WG_DIGITAL_HZTL_12.jpg',\n",
       " 'AERIN_ER_MOODBOARD_LR.jpg']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/denisdunn/Desktop/full_aerin'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATADIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATADIR='/Users/denisdunn/Desktop/full_aerin'\n",
    "CATEGORIES=cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CATEGORIES' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-32871c3e62a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#path =  os.path.join(DATADIR,category)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mcategory\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mCATEGORIES\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATADIR\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcategory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mclass_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCATEGORIES\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcategory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'CATEGORIES' is not defined"
     ]
    }
   ],
   "source": [
    "#path =  os.path.join(DATADIR,category)\n",
    "for category in CATEGORIES:\n",
    "    path =  os.path.join(DATADIR,category)\n",
    "    class_num = CATEGORIES.index(category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_array = cv2.imread('/Users/denisdunn/Desktop/F19_AERIN_LIM_DIGITAL_1_R300.tif')#,cv2.IMREAD_GRAYSCALE)\n",
    "                \n",
    "new_array = cv2.resize(img_array,(IMG_SIZE,IMG_SIZE))\n",
    "test_image = new_array[...,::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_data():\n",
    "    for category in CATEGORIES:\n",
    "        path =  os.path.join(DATADIR,category)\n",
    "        class_num = CATEGORIES.index(category)\n",
    "        for img in os.listdir(path):\n",
    "            try:\n",
    "                img_array = cv2.imread(os.path.join(path,img),cv2.IMREAD_GRAYSCALE)\n",
    "                \n",
    "                new_array = cv2.resize(img_array,(IMG_SIZE,IMG_SIZE))\n",
    "                test_image = new_array[...,::-1]\n",
    "                training_data.append([test_image,class_num])\n",
    "            except Exception as e:\n",
    "                pass\n",
    "create_training_data()\n",
    "#cv2.IMREAD_GRAYSCALE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(training_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.shuffle(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for features, label in training_data:   \n",
    "    X.append(features)\n",
    "    y.append(label)\n",
    "X=np.array(X).reshape(-1,IMG_SIZE,IMG_SIZE,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "y = to_categorical(y,num_classes=44,dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=X/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p=cv2.imread('/Users/denisdunn/Desktop/shirt_test.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = load_img('/Users/denisdunn/Desktop/Screen Shot 2019-12-10 at 1.50.12 PM.png', target_size=(224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = img_to_array(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = preprocess_input(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# define a new model with random weights and 10 classes\n",
    "new_input = Input(shape=(224, 224, 3))\n",
    "model = VGG16( weights='imagenet',input_shape=new_input, classes=44,include_top=False)\n",
    "keras.layers.Dense(64, activation='relu'),\n",
    "keras.layers.Dense(44, activation='softmax'),\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "es=EarlyStopping(patience=5, monitor='loss',verbose=1)\n",
    "\n",
    "model.fit(X, y, epochs=1000,callbacks=[es],validation_split=.1)\n",
    "\n",
    "test_loss, test_acc = model.evaluate(X, y)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "keras.layers.Conv2D(input_shape=(224,224,3),filters=64,kernel_size=(3,3),padding=\"same\", activation=\"relu\"),\n",
    "keras.layers.Conv2D(filters=64,kernel_size=(3,3),padding=\"same\", activation=\"relu\"),\n",
    "keras.layers.ELU(alpha=1.0)\n",
    "keras.layers.MaxPooling2D(pool_size=(2,2),strides=(2,2)),\n",
    "keras.layers.Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"),\n",
    "keras.layers.Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"),\n",
    "keras.layers.MaxPooling2D(pool_size=(2,2),strides=(2,2)),\n",
    "keras.layers.Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"),\n",
    "keras.layers.Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"),\n",
    "keras.layers.Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"),\n",
    "keras.layers.MaxPooling2D(pool_size=(2,2),strides=(2,2)),\n",
    "keras.layers.Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"),\n",
    "keras.layers.Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"),\n",
    "keras.layers.Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"),\n",
    "keras.layers.MaxPooling2D(pool_size=(2,2),strides=(2,2)),\n",
    "keras.layers.Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"),\n",
    "keras.layers.Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"),\n",
    "keras.layers.Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"),\n",
    "keras.layers.MaxPooling2D(pool_size=(2,2),strides=(2,2)),\n",
    "keras.layers.Flatten(),\n",
    "keras.layers.Dense(4096,activation=\"relu\"),\n",
    "keras.layers.Dense(4096,activation=\"relu\"),\n",
    "keras.layers.Dense(44, activation=\"softmax\"),\n",
    "])\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "es=EarlyStopping(patience=3, monitor='loss',verbose=1)\n",
    "\n",
    "model.fit(X, y, epochs=500,callbacks=[es],batch_size=32,validation_split=.1)\n",
    "\n",
    "test_loss, test_acc = model.evaluate(X, y)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out=model.predict(image)\n",
    "decode_predictions(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE=224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#model.compile(loss='categorical_crossentropy',optimizer = 'adam',metrics=['accuracy'])\n",
    "#model.fit(X,y,epochs=10,validation_split=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the data augmentation\n",
    "train_datagen.fit(x_train)\n",
    "\n",
    "# setup generator\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        directory='/Users/denisdunn/Desktop/'+'full_aerin',\n",
    "        target_size=(IMG_SIZE, IMG_SIZE),\n",
    "        color_mode='rgb',\n",
    "        batch_size=1,\n",
    "        class_mode='categorical')\n",
    "\n",
    "# train model\n",
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=nb_train_samples,\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator, # optional - if used needs to be defined\n",
    "    validation_steps=nb_validation_samples) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "from keras import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=load_model('/Users/denisdunn/estee_lauder/weights-improvement-92-0.70.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "VGG16=VGG16(include_top=False, weights='imagenet',input_shape=(224,224,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.vgg16 import preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications import resnet50\n",
    "resnet50 = resnet50.ResNet50(include_top=True, weights='imagenet')\n",
    "#model.load_weights('resnet50_weights_tf_dim_ordering_tf_kernels.h5')\n",
    "#model.compile(optimizer='rmsprop', loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 0\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "VGG16.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "#25088\n",
    "model.add(VGG16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg16 (Model)                (None, 7, 7, 512)         14714688  \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 0\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(layers.Dense(44, activation='softmax')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Depth of input (512) is not a multiple of input depth of filter (3) for 'vgg16_7/block1_conv1/convolution' (op: 'Conv2D') with input shapes: [?,7,7,512], [3,3,3,64].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[1;32m   1618\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1619\u001b[0;31m     \u001b[0mc_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_FinishOperation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_desc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1620\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Depth of input (512) is not a multiple of input depth of filter (3) for 'vgg16_7/block1_conv1/convolution' (op: 'Conv2D') with input shapes: [?,7,7,512], [3,3,3,64].",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-9e1d3f434173>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m   \u001b[0;31m# go through until last layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m#layer.name = layer.name + str(\"_2\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;31m#model.add(keras.layers.Flatten()),\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m#model.add(keras.layers.Dense(4086, activation='elu')),\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/keras/engine/sequential.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(self, layer)\u001b[0m\n\u001b[1;32m    180\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_source_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m             \u001b[0moutput_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 raise TypeError('All layers in a Sequential model '\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36msymbolic_fn_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_SYMBOLIC_SCOPE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mget_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0;31m# Actually call the layer,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m             \u001b[0;31m# collecting output(s), mask(s), and shape(s).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m             \u001b[0moutput_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprevious_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/keras/engine/network.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, mask)\u001b[0m\n\u001b[1;32m    581\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output_tensor_cache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m             \u001b[0moutput_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_internal_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutput_tensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/keras/engine/network.py\u001b[0m in \u001b[0;36mrun_internal_graph\u001b[0;34m(self, inputs, masks)\u001b[0m\n\u001b[1;32m    738\u001b[0m                                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mask'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcomputed_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m                             output_tensors = to_list(\n\u001b[0;32m--> 740\u001b[0;31m                                 layer.call(computed_tensor, **kwargs))\n\u001b[0m\u001b[1;32m    741\u001b[0m                             output_masks = layer.compute_mask(computed_tensor,\n\u001b[1;32m    742\u001b[0m                                                               computed_mask)\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/keras/layers/convolutional.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    169\u001b[0m                 \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m                 dilation_rate=self.dilation_rate)\n\u001b[0m\u001b[1;32m    172\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrank\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m             outputs = K.conv3d(\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mconv2d\u001b[0;34m(x, kernel, strides, padding, data_format, dilation_rate)\u001b[0m\n\u001b[1;32m   3715\u001b[0m         \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3716\u001b[0m         \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf_data_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3717\u001b[0;31m         **kwargs)\n\u001b[0m\u001b[1;32m   3718\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdata_format\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'channels_first'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtf_data_format\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'NHWC'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3719\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# NHWC -> NCHW\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/nn_ops.py\u001b[0m in \u001b[0;36mconvolution_v2\u001b[0;34m(input, filters, strides, padding, data_format, dilations, name)\u001b[0m\n\u001b[1;32m    916\u001b[0m       \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mdilations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdilations\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 918\u001b[0;31m       name=name)\n\u001b[0m\u001b[1;32m    919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/nn_ops.py\u001b[0m in \u001b[0;36mconvolution_internal\u001b[0;34m(input, filters, strides, padding, data_format, dilations, name, call_from_convolution)\u001b[0m\n\u001b[1;32m   1008\u001b[0m           \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1009\u001b[0m           \u001b[0mdilations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdilations\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1010\u001b[0;31m           name=name)\n\u001b[0m\u001b[1;32m   1011\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1012\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mchannel_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_nn_ops.py\u001b[0m in \u001b[0;36mconv2d\u001b[0;34m(input, filter, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name)\u001b[0m\n\u001b[1;32m    967\u001b[0m                   \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    968\u001b[0m                   \u001b[0mexplicit_paddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexplicit_paddings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 969\u001b[0;31m                   data_format=data_format, dilations=dilations, name=name)\n\u001b[0m\u001b[1;32m    970\u001b[0m   \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    971\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0m_execute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmust_record_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    740\u001b[0m       op = g._create_op_internal(op_type_name, inputs, dtypes=None,\n\u001b[1;32m    741\u001b[0m                                  \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 742\u001b[0;31m                                  attrs=attr_protos, op_def=op_def)\n\u001b[0m\u001b[1;32m    743\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    744\u001b[0m     \u001b[0;31m# `outputs` is returned as a separate return value so that the output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py\u001b[0m in \u001b[0;36m_create_op_internal\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[1;32m    593\u001b[0m     return super(FuncGraph, self)._create_op_internal(  # pylint: disable=protected-access\n\u001b[1;32m    594\u001b[0m         \u001b[0mop_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 595\u001b[0;31m         compute_device)\n\u001b[0m\u001b[1;32m    596\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcapture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_op_internal\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[1;32m   3320\u001b[0m           \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3321\u001b[0m           \u001b[0moriginal_op\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_default_original_op\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3322\u001b[0;31m           op_def=op_def)\n\u001b[0m\u001b[1;32m   3323\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_op_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3324\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\u001b[0m\n\u001b[1;32m   1784\u001b[0m           op_def, inputs, node_def.attr)\n\u001b[1;32m   1785\u001b[0m       self._c_op = _create_c_op(self._graph, node_def, grouped_inputs,\n\u001b[0;32m-> 1786\u001b[0;31m                                 control_input_ops)\n\u001b[0m\u001b[1;32m   1787\u001b[0m       \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m     \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[1;32m   1620\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1621\u001b[0m     \u001b[0;31m# Convert to ValueError for backwards compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1622\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1623\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1624\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mc_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Depth of input (512) is not a multiple of input depth of filter (3) for 'vgg16_7/block1_conv1/convolution' (op: 'Conv2D') with input shapes: [?,7,7,512], [3,3,3,64]."
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "#25088\n",
    "#build(VGG16)\n",
    "model.add(VGG16)\n",
    "#model = model_2.Sequential()\n",
    "#model = keras.Sequential()\n",
    "for layer in model.layers:\n",
    "    layer.trainable = False   # go through until last layer\n",
    "    #layer.name = layer.name + str(\"_2\")\n",
    "    model.add(layer)\n",
    "#model.add(keras.layers.Flatten()),\n",
    "#model.add(keras.layers.Dense(4086, activation='elu')),\n",
    "#model.add(layers.Dense(528, activation='elu')),\n",
    "#model.add(layers.Dense(128, activation='elu')),     \n",
    "#model.add(layers.Dense(44, activation='softmax')) \n",
    "\n",
    "# Add the vgg convolutional base model\n",
    "# Add new layers\n",
    "\n",
    "#model.add(layers.Flatten(input_shape=(20,20,1)))\n",
    "\n",
    "#model.add(layers.Dense(1024, activation='relu'))\n",
    "\n",
    "#model.add(layers.Dropout(0.5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-5f15418b3570>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 484 images belonging to 44 classes.\n",
      "Epoch 1/1000\n",
      "2000/2000 [==============================] - 2571s 1s/step - loss: 3.1270 - accuracy: 0.4585\n",
      "\n",
      "Epoch 00001: accuracy improved from -inf to 0.45850, saving model to weights-improvement-01-0.46.hdf5\n",
      "Epoch 2/1000\n",
      "2000/2000 [==============================] - 2625s 1s/step - loss: 2.9483 - accuracy: 0.4430\n",
      "\n",
      "Epoch 00002: accuracy did not improve from 0.45850\n",
      "Epoch 3/1000\n",
      "2000/2000 [==============================] - 2609s 1s/step - loss: 2.8797 - accuracy: 0.4115\n",
      "\n",
      "Epoch 00003: accuracy did not improve from 0.45850\n",
      "Epoch 4/1000\n",
      "2000/2000 [==============================] - 2746s 1s/step - loss: 2.7921 - accuracy: 0.4335\n",
      "\n",
      "Epoch 00004: accuracy did not improve from 0.45850\n",
      "Epoch 5/1000\n",
      "2000/2000 [==============================] - 2584s 1s/step - loss: 2.7465 - accuracy: 0.4315\n",
      "\n",
      "Epoch 00005: accuracy did not improve from 0.45850\n",
      "Epoch 6/1000\n",
      "2000/2000 [==============================] - 2521s 1s/step - loss: 2.6865 - accuracy: 0.4565\n",
      "\n",
      "Epoch 00006: accuracy did not improve from 0.45850\n",
      "Epoch 7/1000\n",
      "2000/2000 [==============================] - 2532s 1s/step - loss: 2.6594 - accuracy: 0.4640\n",
      "\n",
      "Epoch 00007: accuracy improved from 0.45850 to 0.46400, saving model to weights-improvement-07-0.46.hdf5\n",
      "Epoch 8/1000\n",
      "2000/2000 [==============================] - 2544s 1s/step - loss: 2.6184 - accuracy: 0.4785\n",
      "\n",
      "Epoch 00008: accuracy improved from 0.46400 to 0.47850, saving model to weights-improvement-08-0.48.hdf5\n",
      "Epoch 9/1000\n",
      "2000/2000 [==============================] - 2522s 1s/step - loss: 2.5998 - accuracy: 0.4875\n",
      "\n",
      "Epoch 00009: accuracy improved from 0.47850 to 0.48750, saving model to weights-improvement-09-0.49.hdf5\n",
      "Epoch 10/1000\n",
      "2000/2000 [==============================] - 2532s 1s/step - loss: 2.5473 - accuracy: 0.5000\n",
      "\n",
      "Epoch 00010: accuracy improved from 0.48750 to 0.50000, saving model to weights-improvement-10-0.50.hdf5\n",
      "Epoch 11/1000\n",
      "2000/2000 [==============================] - 2529s 1s/step - loss: 2.5231 - accuracy: 0.5200\n",
      "\n",
      "Epoch 00011: accuracy improved from 0.50000 to 0.52000, saving model to weights-improvement-11-0.52.hdf5\n",
      "Epoch 12/1000\n",
      "2000/2000 [==============================] - 2501s 1s/step - loss: 2.4918 - accuracy: 0.5350\n",
      "\n",
      "Epoch 00012: accuracy improved from 0.52000 to 0.53500, saving model to weights-improvement-12-0.54.hdf5\n",
      "Epoch 13/1000\n",
      "2000/2000 [==============================] - 2527s 1s/step - loss: 2.4500 - accuracy: 0.5445\n",
      "\n",
      "Epoch 00013: accuracy improved from 0.53500 to 0.54450, saving model to weights-improvement-13-0.54.hdf5\n",
      "Epoch 14/1000\n",
      "2000/2000 [==============================] - 2547s 1s/step - loss: 2.4378 - accuracy: 0.5625\n",
      "\n",
      "Epoch 00014: accuracy improved from 0.54450 to 0.56250, saving model to weights-improvement-14-0.56.hdf5\n",
      "Epoch 15/1000\n",
      "2000/2000 [==============================] - 2539s 1s/step - loss: 2.4168 - accuracy: 0.5715\n",
      "\n",
      "Epoch 00015: accuracy improved from 0.56250 to 0.57150, saving model to weights-improvement-15-0.57.hdf5\n",
      "Epoch 16/1000\n",
      "2000/2000 [==============================] - 2506s 1s/step - loss: 2.4054 - accuracy: 0.5710\n",
      "\n",
      "Epoch 00016: accuracy did not improve from 0.57150\n",
      "Epoch 17/1000\n",
      "2000/2000 [==============================] - 2546s 1s/step - loss: 2.3645 - accuracy: 0.5830\n",
      "\n",
      "Epoch 00017: accuracy improved from 0.57150 to 0.58300, saving model to weights-improvement-17-0.58.hdf5\n",
      "Epoch 18/1000\n",
      "2000/2000 [==============================] - 2546s 1s/step - loss: 2.3567 - accuracy: 0.5845\n",
      "\n",
      "Epoch 00018: accuracy improved from 0.58300 to 0.58450, saving model to weights-improvement-18-0.58.hdf5\n",
      "Epoch 19/1000\n",
      "2000/2000 [==============================] - 2518s 1s/step - loss: 2.3408 - accuracy: 0.5960\n",
      "\n",
      "Epoch 00019: accuracy improved from 0.58450 to 0.59600, saving model to weights-improvement-19-0.60.hdf5\n",
      "Epoch 20/1000\n",
      "2000/2000 [==============================] - 2523s 1s/step - loss: 2.3232 - accuracy: 0.6185\n",
      "\n",
      "Epoch 00020: accuracy improved from 0.59600 to 0.61850, saving model to weights-improvement-20-0.62.hdf5\n",
      "Epoch 21/1000\n",
      "2000/2000 [==============================] - 2522s 1s/step - loss: 2.3186 - accuracy: 0.6120\n",
      "\n",
      "Epoch 00021: accuracy did not improve from 0.61850\n",
      "Epoch 22/1000\n",
      "2000/2000 [==============================] - 2512s 1s/step - loss: 2.2997 - accuracy: 0.6270\n",
      "\n",
      "Epoch 00022: accuracy improved from 0.61850 to 0.62700, saving model to weights-improvement-22-0.63.hdf5\n",
      "Epoch 23/1000\n",
      "2000/2000 [==============================] - 2539s 1s/step - loss: 2.2799 - accuracy: 0.6350\n",
      "\n",
      "Epoch 00023: accuracy improved from 0.62700 to 0.63500, saving model to weights-improvement-23-0.63.hdf5\n",
      "Epoch 24/1000\n",
      "2000/2000 [==============================] - 2521s 1s/step - loss: 2.2797 - accuracy: 0.6330\n",
      "\n",
      "Epoch 00024: accuracy did not improve from 0.63500\n",
      "Epoch 25/1000\n",
      "2000/2000 [==============================] - 2516s 1s/step - loss: 2.2747 - accuracy: 0.6305\n",
      "\n",
      "Epoch 00025: accuracy did not improve from 0.63500\n",
      "Epoch 26/1000\n",
      "2000/2000 [==============================] - 2517s 1s/step - loss: 2.2248 - accuracy: 0.6420\n",
      "\n",
      "Epoch 00026: accuracy improved from 0.63500 to 0.64200, saving model to weights-improvement-26-0.64.hdf5\n",
      "Epoch 27/1000\n",
      "2000/2000 [==============================] - 2529s 1s/step - loss: 2.2766 - accuracy: 0.6320\n",
      "\n",
      "Epoch 00027: accuracy did not improve from 0.64200\n",
      "Epoch 28/1000\n",
      "2000/2000 [==============================] - 2521s 1s/step - loss: 2.2172 - accuracy: 0.6615\n",
      "\n",
      "Epoch 00028: accuracy improved from 0.64200 to 0.66150, saving model to weights-improvement-28-0.66.hdf5\n",
      "Epoch 29/1000\n",
      "2000/2000 [==============================] - 2528s 1s/step - loss: 2.2235 - accuracy: 0.6615\n",
      "\n",
      "Epoch 00029: accuracy did not improve from 0.66150\n",
      "Epoch 30/1000\n",
      "2000/2000 [==============================] - 2531s 1s/step - loss: 2.2179 - accuracy: 0.6545\n",
      "\n",
      "Epoch 00030: accuracy did not improve from 0.66150\n",
      "Epoch 31/1000\n",
      "2000/2000 [==============================] - 2546s 1s/step - loss: 2.2232 - accuracy: 0.6520\n",
      "\n",
      "Epoch 00031: accuracy did not improve from 0.66150\n",
      "Epoch 32/1000\n",
      "2000/2000 [==============================] - 2543s 1s/step - loss: 2.2164 - accuracy: 0.6550\n",
      "\n",
      "Epoch 00032: accuracy did not improve from 0.66150\n",
      "Epoch 33/1000\n",
      "2000/2000 [==============================] - 2531s 1s/step - loss: 2.1952 - accuracy: 0.6645\n",
      "\n",
      "Epoch 00033: accuracy improved from 0.66150 to 0.66450, saving model to weights-improvement-33-0.66.hdf5\n",
      "Epoch 34/1000\n",
      "2000/2000 [==============================] - 2559s 1s/step - loss: 2.1960 - accuracy: 0.6695\n",
      "\n",
      "Epoch 00034: accuracy improved from 0.66450 to 0.66950, saving model to weights-improvement-34-0.67.hdf5\n",
      "Epoch 35/1000\n",
      "2000/2000 [==============================] - 2540s 1s/step - loss: 2.1947 - accuracy: 0.6755\n",
      "\n",
      "Epoch 00035: accuracy improved from 0.66950 to 0.67550, saving model to weights-improvement-35-0.68.hdf5\n",
      "Epoch 36/1000\n",
      "2000/2000 [==============================] - 2551s 1s/step - loss: 2.1876 - accuracy: 0.6735\n",
      "\n",
      "Epoch 00036: accuracy did not improve from 0.67550\n",
      "Epoch 37/1000\n",
      "2000/2000 [==============================] - 2567s 1s/step - loss: 2.1942 - accuracy: 0.6660\n",
      "\n",
      "Epoch 00037: accuracy did not improve from 0.67550\n",
      "Epoch 38/1000\n",
      "2000/2000 [==============================] - 2557s 1s/step - loss: 2.1754 - accuracy: 0.6795\n",
      "\n",
      "Epoch 00038: accuracy improved from 0.67550 to 0.67950, saving model to weights-improvement-38-0.68.hdf5\n",
      "Epoch 39/1000\n",
      "2000/2000 [==============================] - 2529s 1s/step - loss: 2.1722 - accuracy: 0.6730\n",
      "\n",
      "Epoch 00039: accuracy did not improve from 0.67950\n",
      "Epoch 40/1000\n",
      "2000/2000 [==============================] - 2540s 1s/step - loss: 2.1790 - accuracy: 0.6805\n",
      "\n",
      "Epoch 00040: accuracy improved from 0.67950 to 0.68050, saving model to weights-improvement-40-0.68.hdf5\n",
      "Epoch 41/1000\n",
      "2000/2000 [==============================] - 2560s 1s/step - loss: 2.1705 - accuracy: 0.6670\n",
      "\n",
      "Epoch 00041: accuracy did not improve from 0.68050\n",
      "Epoch 42/1000\n",
      "2000/2000 [==============================] - 2554s 1s/step - loss: 2.1650 - accuracy: 0.6740\n",
      "\n",
      "Epoch 00042: accuracy did not improve from 0.68050\n",
      "Epoch 43/1000\n",
      "2000/2000 [==============================] - 2583s 1s/step - loss: 2.1577 - accuracy: 0.6805\n",
      "\n",
      "Epoch 00043: accuracy did not improve from 0.68050\n",
      "Epoch 44/1000\n",
      "2000/2000 [==============================] - 2545s 1s/step - loss: 2.1711 - accuracy: 0.6840\n",
      "\n",
      "Epoch 00044: accuracy improved from 0.68050 to 0.68400, saving model to weights-improvement-44-0.68.hdf5\n",
      "Epoch 45/1000\n",
      "2000/2000 [==============================] - 2557s 1s/step - loss: 2.1573 - accuracy: 0.6800\n",
      "\n",
      "Epoch 00045: accuracy did not improve from 0.68400\n",
      "Epoch 46/1000\n",
      "2000/2000 [==============================] - 2699s 1s/step - loss: 2.1514 - accuracy: 0.6815\n",
      "\n",
      "Epoch 00046: accuracy did not improve from 0.68400\n",
      "Epoch 47/1000\n",
      "2000/2000 [==============================] - 3052s 2s/step - loss: 2.1588 - accuracy: 0.6875\n",
      "\n",
      "Epoch 00047: accuracy improved from 0.68400 to 0.68750, saving model to weights-improvement-47-0.69.hdf5\n",
      "Epoch 48/1000\n",
      "2000/2000 [==============================] - 3077s 2s/step - loss: 2.1437 - accuracy: 0.6815\n",
      "\n",
      "Epoch 00048: accuracy did not improve from 0.68750\n",
      "Epoch 49/1000\n",
      "2000/2000 [==============================] - 3069s 2s/step - loss: 2.1446 - accuracy: 0.6775\n",
      "\n",
      "Epoch 00049: accuracy did not improve from 0.68750\n",
      "Epoch 50/1000\n",
      "2000/2000 [==============================] - 3086s 2s/step - loss: 2.1386 - accuracy: 0.6840\n",
      "\n",
      "Epoch 00050: accuracy did not improve from 0.68750\n",
      "Epoch 51/1000\n",
      "2000/2000 [==============================] - 2764s 1s/step - loss: 2.1336 - accuracy: 0.6885\n",
      "\n",
      "Epoch 00051: accuracy improved from 0.68750 to 0.68850, saving model to weights-improvement-51-0.69.hdf5\n",
      "Epoch 52/1000\n",
      "2000/2000 [==============================] - 2498s 1s/step - loss: 2.1429 - accuracy: 0.6855\n",
      "\n",
      "Epoch 00052: accuracy did not improve from 0.68850\n",
      "Epoch 53/1000\n",
      "2000/2000 [==============================] - 2545s 1s/step - loss: 2.1298 - accuracy: 0.6845\n",
      "\n",
      "Epoch 00053: accuracy did not improve from 0.68850\n",
      "Epoch 54/1000\n",
      "2000/2000 [==============================] - 2527s 1s/step - loss: 2.1390 - accuracy: 0.6905\n",
      "\n",
      "Epoch 00054: accuracy improved from 0.68850 to 0.69050, saving model to weights-improvement-54-0.69.hdf5\n",
      "Epoch 55/1000\n",
      "2000/2000 [==============================] - 2555s 1s/step - loss: 2.1335 - accuracy: 0.6825\n",
      "\n",
      "Epoch 00055: accuracy did not improve from 0.69050\n",
      "Epoch 56/1000\n",
      "2000/2000 [==============================] - 2517s 1s/step - loss: 2.1263 - accuracy: 0.6900\n",
      "\n",
      "Epoch 00056: accuracy did not improve from 0.69050\n",
      "Epoch 57/1000\n",
      "2000/2000 [==============================] - 2513s 1s/step - loss: 2.1352 - accuracy: 0.6865\n",
      "\n",
      "Epoch 00057: accuracy did not improve from 0.69050\n",
      "Epoch 58/1000\n",
      "2000/2000 [==============================] - 2546s 1s/step - loss: 2.1052 - accuracy: 0.6840\n",
      "\n",
      "Epoch 00058: accuracy did not improve from 0.69050\n",
      "Epoch 59/1000\n",
      "2000/2000 [==============================] - 2552s 1s/step - loss: 2.1422 - accuracy: 0.6860\n",
      "\n",
      "Epoch 00059: accuracy did not improve from 0.69050\n",
      "Epoch 60/1000\n",
      "2000/2000 [==============================] - 2543s 1s/step - loss: 2.1196 - accuracy: 0.6850\n",
      "\n",
      "Epoch 00060: accuracy did not improve from 0.69050\n",
      "Epoch 61/1000\n",
      "2000/2000 [==============================] - 2547s 1s/step - loss: 2.1227 - accuracy: 0.6860\n",
      "\n",
      "Epoch 00061: accuracy did not improve from 0.69050\n",
      "Epoch 62/1000\n",
      "2000/2000 [==============================] - 2543s 1s/step - loss: 2.1268 - accuracy: 0.6920\n",
      "\n",
      "Epoch 00062: accuracy improved from 0.69050 to 0.69200, saving model to weights-improvement-62-0.69.hdf5\n",
      "Epoch 63/1000\n",
      "2000/2000 [==============================] - 2541s 1s/step - loss: 2.1169 - accuracy: 0.6845\n",
      "\n",
      "Epoch 00063: accuracy did not improve from 0.69200\n",
      "Epoch 64/1000\n",
      "2000/2000 [==============================] - 2546s 1s/step - loss: 2.1185 - accuracy: 0.6890\n",
      "\n",
      "Epoch 00064: accuracy did not improve from 0.69200\n",
      "Epoch 65/1000\n",
      "2000/2000 [==============================] - 2545s 1s/step - loss: 2.1102 - accuracy: 0.6940\n",
      "\n",
      "Epoch 00065: accuracy improved from 0.69200 to 0.69400, saving model to weights-improvement-65-0.69.hdf5\n",
      "Epoch 66/1000\n",
      "2000/2000 [==============================] - 2592s 1s/step - loss: 2.1209 - accuracy: 0.6815\n",
      "\n",
      "Epoch 00066: accuracy did not improve from 0.69400\n",
      "Epoch 67/1000\n",
      "2000/2000 [==============================] - 2577s 1s/step - loss: 2.1126 - accuracy: 0.6880\n",
      "\n",
      "Epoch 00067: accuracy did not improve from 0.69400\n",
      "Epoch 68/1000\n",
      "2000/2000 [==============================] - 2527s 1s/step - loss: 2.1175 - accuracy: 0.6890\n",
      "\n",
      "Epoch 00068: accuracy did not improve from 0.69400\n",
      "Epoch 69/1000\n",
      "2000/2000 [==============================] - 2569s 1s/step - loss: 2.1186 - accuracy: 0.6880\n",
      "\n",
      "Epoch 00069: accuracy did not improve from 0.69400\n",
      "Epoch 70/1000\n",
      "2000/2000 [==============================] - 2547s 1s/step - loss: 2.1157 - accuracy: 0.6920\n",
      "\n",
      "Epoch 00070: accuracy did not improve from 0.69400\n",
      "Epoch 71/1000\n",
      "2000/2000 [==============================] - 2543s 1s/step - loss: 2.1125 - accuracy: 0.6895\n",
      "\n",
      "Epoch 00071: accuracy did not improve from 0.69400\n",
      "Epoch 72/1000\n",
      "2000/2000 [==============================] - 2512s 1s/step - loss: 2.1070 - accuracy: 0.6925\n",
      "\n",
      "Epoch 00072: accuracy did not improve from 0.69400\n",
      "Epoch 73/1000\n",
      "2000/2000 [==============================] - 2552s 1s/step - loss: 2.1199 - accuracy: 0.6890\n",
      "\n",
      "Epoch 00073: accuracy did not improve from 0.69400\n",
      "Epoch 74/1000\n",
      "2000/2000 [==============================] - 2515s 1s/step - loss: 2.1033 - accuracy: 0.6975\n",
      "\n",
      "Epoch 00074: accuracy improved from 0.69400 to 0.69750, saving model to weights-improvement-74-0.70.hdf5\n",
      "Epoch 75/1000\n",
      "2000/2000 [==============================] - 2526s 1s/step - loss: 2.1101 - accuracy: 0.6915\n",
      "\n",
      "Epoch 00075: accuracy did not improve from 0.69750\n",
      "Epoch 76/1000\n",
      "2000/2000 [==============================] - 2501s 1s/step - loss: 2.1031 - accuracy: 0.6925\n",
      "\n",
      "Epoch 00076: accuracy did not improve from 0.69750\n",
      "Epoch 77/1000\n",
      "2000/2000 [==============================] - 2535s 1s/step - loss: 2.1069 - accuracy: 0.6930\n",
      "\n",
      "Epoch 00077: accuracy did not improve from 0.69750\n",
      "Epoch 78/1000\n",
      "2000/2000 [==============================] - 2542s 1s/step - loss: 2.1086 - accuracy: 0.6960\n",
      "\n",
      "Epoch 00078: accuracy did not improve from 0.69750\n",
      "Epoch 79/1000\n",
      "2000/2000 [==============================] - 2508s 1s/step - loss: 2.0952 - accuracy: 0.6980\n",
      "\n",
      "Epoch 00079: accuracy improved from 0.69750 to 0.69800, saving model to weights-improvement-79-0.70.hdf5\n",
      "Epoch 80/1000\n",
      "2000/2000 [==============================] - 2524s 1s/step - loss: 2.0963 - accuracy: 0.6890\n",
      "\n",
      "Epoch 00080: accuracy did not improve from 0.69800\n",
      "Epoch 81/1000\n",
      "2000/2000 [==============================] - 2561s 1s/step - loss: 2.0974 - accuracy: 0.6970\n",
      "\n",
      "Epoch 00081: accuracy did not improve from 0.69800\n",
      "Epoch 82/1000\n",
      "2000/2000 [==============================] - 2535s 1s/step - loss: 2.1089 - accuracy: 0.6975\n",
      "\n",
      "Epoch 00082: accuracy did not improve from 0.69800\n",
      "Epoch 83/1000\n",
      "2000/2000 [==============================] - 2545s 1s/step - loss: 2.0988 - accuracy: 0.6965\n",
      "\n",
      "Epoch 00083: accuracy did not improve from 0.69800\n",
      "Epoch 84/1000\n",
      "2000/2000 [==============================] - 2578s 1s/step - loss: 2.0960 - accuracy: 0.6955\n",
      "\n",
      "Epoch 00084: accuracy did not improve from 0.69800\n",
      "Epoch 85/1000\n",
      "2000/2000 [==============================] - 2539s 1s/step - loss: 2.0991 - accuracy: 0.6945\n",
      "\n",
      "Epoch 00085: accuracy did not improve from 0.69800\n",
      "Epoch 86/1000\n",
      "2000/2000 [==============================] - 2502s 1s/step - loss: 2.0936 - accuracy: 0.6940\n",
      "\n",
      "Epoch 00086: accuracy did not improve from 0.69800\n",
      "Epoch 87/1000\n",
      "2000/2000 [==============================] - 2530s 1s/step - loss: 2.0995 - accuracy: 0.6925\n",
      "\n",
      "Epoch 00087: accuracy did not improve from 0.69800\n",
      "Epoch 88/1000\n",
      "2000/2000 [==============================] - 2531s 1s/step - loss: 2.1001 - accuracy: 0.6950\n",
      "\n",
      "Epoch 00088: accuracy did not improve from 0.69800\n",
      "Epoch 89/1000\n",
      "2000/2000 [==============================] - 2553s 1s/step - loss: 2.1015 - accuracy: 0.6915\n",
      "\n",
      "Epoch 00089: accuracy did not improve from 0.69800\n",
      "Epoch 90/1000\n",
      "2000/2000 [==============================] - 2533s 1s/step - loss: 2.0914 - accuracy: 0.6995\n",
      "\n",
      "Epoch 00090: accuracy improved from 0.69800 to 0.69950, saving model to weights-improvement-90-0.70.hdf5\n",
      "Epoch 91/1000\n",
      "2000/2000 [==============================] - 2531s 1s/step - loss: 2.0957 - accuracy: 0.6920\n",
      "\n",
      "Epoch 00091: accuracy did not improve from 0.69950\n",
      "Epoch 92/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 2552s 1s/step - loss: 2.0878 - accuracy: 0.7025\n",
      "\n",
      "Epoch 00092: accuracy improved from 0.69950 to 0.70250, saving model to weights-improvement-92-0.70.hdf5\n",
      "Epoch 93/1000\n",
      "2000/2000 [==============================] - 2574s 1s/step - loss: 2.1038 - accuracy: 0.6865\n",
      "\n",
      "Epoch 00093: accuracy did not improve from 0.70250\n",
      "Epoch 94/1000\n",
      "2000/2000 [==============================] - 2562s 1s/step - loss: 2.0912 - accuracy: 0.6940\n",
      "\n",
      "Epoch 00094: accuracy did not improve from 0.70250\n",
      "Epoch 95/1000\n",
      "2000/2000 [==============================] - 2551s 1s/step - loss: 2.0959 - accuracy: 0.6945\n",
      "\n",
      "Epoch 00095: accuracy did not improve from 0.70250\n",
      "Epoch 96/1000\n",
      "2000/2000 [==============================] - 2569s 1s/step - loss: 2.0873 - accuracy: 0.7005\n",
      "\n",
      "Epoch 00096: accuracy did not improve from 0.70250\n",
      "Epoch 97/1000\n",
      "2000/2000 [==============================] - 2566s 1s/step - loss: 2.0920 - accuracy: 0.6895\n",
      "\n",
      "Epoch 00097: accuracy did not improve from 0.70250\n",
      "Epoch 98/1000\n",
      "2000/2000 [==============================] - 2586s 1s/step - loss: 2.0938 - accuracy: 0.6930\n",
      "\n",
      "Epoch 00098: accuracy did not improve from 0.70250\n",
      "Epoch 99/1000\n",
      "2000/2000 [==============================] - 2534s 1s/step - loss: 2.0907 - accuracy: 0.6960\n",
      "\n",
      "Epoch 00099: accuracy did not improve from 0.70250\n",
      "Epoch 100/1000\n",
      "2000/2000 [==============================] - 2529s 1s/step - loss: 2.0948 - accuracy: 0.6945\n",
      "\n",
      "Epoch 00100: accuracy did not improve from 0.70250\n",
      "Epoch 101/1000\n",
      "2000/2000 [==============================] - 2549s 1s/step - loss: 2.0868 - accuracy: 0.6980\n",
      "\n",
      "Epoch 00101: accuracy did not improve from 0.70250\n",
      "Epoch 102/1000\n",
      "2000/2000 [==============================] - 2549s 1s/step - loss: 2.0821 - accuracy: 0.6995\n",
      "\n",
      "Epoch 00102: accuracy did not improve from 0.70250\n",
      "Epoch 103/1000\n",
      "2000/2000 [==============================] - 2531s 1s/step - loss: 2.0977 - accuracy: 0.6855\n",
      "\n",
      "Epoch 00103: accuracy did not improve from 0.70250\n",
      "Epoch 104/1000\n",
      "2000/2000 [==============================] - 2580s 1s/step - loss: 2.0891 - accuracy: 0.6965\n",
      "\n",
      "Epoch 00104: accuracy did not improve from 0.70250\n",
      "Epoch 105/1000\n",
      "2000/2000 [==============================] - 2558s 1s/step - loss: 2.0883 - accuracy: 0.6945\n",
      "\n",
      "Epoch 00105: accuracy did not improve from 0.70250\n",
      "Epoch 106/1000\n",
      "2000/2000 [==============================] - 2554s 1s/step - loss: 2.0902 - accuracy: 0.6905\n",
      "\n",
      "Epoch 00106: accuracy did not improve from 0.70250\n",
      "Epoch 107/1000\n",
      "2000/2000 [==============================] - 2572s 1s/step - loss: 2.0906 - accuracy: 0.6895\n",
      "\n",
      "Epoch 00107: accuracy did not improve from 0.70250\n",
      "Epoch 108/1000\n",
      "2000/2000 [==============================] - 2581s 1s/step - loss: 2.0907 - accuracy: 0.6895\n",
      "\n",
      "Epoch 00108: accuracy did not improve from 0.70250\n",
      "Epoch 109/1000\n",
      " 811/2000 [===========>..................] - ETA: 25:07 - loss: 2.0880 - accuracy: 0.6893"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-d74ed309a666>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m model.fit_generator(train_generator,\n\u001b[1;32m     18\u001b[0m     \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     epochs=1000,callbacks=[checkpoint]) \n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1730\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1731\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1732\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    218\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                                             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m                                             reset_metrics=False)\n\u001b[0m\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m   1512\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1514\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1516\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3725\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3726\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3727\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3729\u001b[0m     \u001b[0;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1549\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1550\u001b[0m     \"\"\"\n\u001b[0;32m-> 1551\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1553\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1589\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[1;32m   1590\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[0;32m-> 1591\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1592\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1593\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1690\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1692\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sgd= optimizers.SGD(lr=.000001,)\n",
    "model.compile(optimizer=sgd,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "filepath=\"weights-improvement-{epoch:02d}-{accuracy:.2f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "#callbacks_list = [checkpoint]\n",
    "#es=EarlyStopping(patience=15, monitor='loss',verbose=1)\n",
    "#datagen.fit(X)\n",
    "train_generator = datagen.flow_from_directory(\n",
    "        directory='/Users/denisdunn/Desktop/'+'full_aerin',\n",
    "        target_size=(IMG_SIZE, IMG_SIZE),\n",
    "        color_mode='rgb',\n",
    "        batch_size=1,\n",
    "        class_mode='categorical')\n",
    "\n",
    "model.fit_generator(train_generator,\n",
    "    steps_per_epoch=2000,\n",
    "    epochs=1000,callbacks=[checkpoint]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    #keras.layers.Conv2D(input_shape=(50,50,1),filters=64,kernel_size=(1,1), activation='relu',strides=(1,1),padding='valid'),\n",
    "    #keras.layers.BatchNormalization(axis=-1,momentum=.99,epsilon=.001,center=True,scale=True,beta_initializer='zeros',gamma_initializer='ones',moving_mean_initializer='zero',moving_variance_initializer='ones'),\n",
    "    #keras.layers.Conv2D(filters=256,kernel_size=(5,5), strides=(3,3),padding='valid', activation='elu'),\n",
    "    #keras.layers.BatchNormalization(axis=-1),#momentum=.99,epsilon=.001,center=True,scale=True,beta_initializer='zeros',gamma_initializer='ones',moving_mean_initializer='zero',moving_variance_initializer='ones'),\n",
    "    #keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(1,1), padding='valid'),\n",
    "    keras.layers.Flatten(input_shape=(20,20,1)),\n",
    "    #keras.layers.Dense(4096),\n",
    "    #keras.layers.ELU(alpha=1.0),\n",
    "    keras.layers.Dense(528, activation='elu'),\n",
    "    #keras.layers.BatchNormalization(axis=-1),#momentum=.99,epsilon=.001,center=True,scale=True,beta_initializer='zeros',gamma_initializer='ones',moving_mean_initializer='zero',moving_variance_initializer='ones'),\n",
    "    keras.layers.Dense(528, activation='elu'),\n",
    "    #keras.layers.Dropout(.3),\n",
    "    keras.layers.Dense(128, activation='elu'),\n",
    "    #keras.layers.Dropout(.5),\n",
    "    keras.layers.Dense(64, activation='elu'),\n",
    "    keras.layers.Dense(44, activation='softmax'),\n",
    "])\n",
    "sgd= optimizers.SGD(lr=.1,decay=1e-1, momentum=.9,nesterov=True)\n",
    "model.compile(optimizer=\"SGD\",\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "filepath=\"weights-improvement-{epoch:02d}-{accuracy:.2f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "#callbacks_list = [checkpoint]\n",
    "#es=EarlyStopping(patience=15, monitor='loss',verbose=1)\n",
    "#datagen.fit(X)\n",
    "train_generator = datagen.flow_from_directory(\n",
    "        directory='/Users/denisdunn/Desktop/'+'full_aerin',\n",
    "        target_size=(IMG_SIZE, IMG_SIZE),\n",
    "        color_mode='grayscale',\n",
    "        batch_size=1,\n",
    "        class_mode='categorical')\n",
    "\n",
    "model.fit_generator(train_generator,\n",
    "    steps_per_epoch=2000,\n",
    "    epochs=1000,callbacks=[checkpoint]) \n",
    "#model.fit_generator(datagen.flow(X,y,batch_size=4),\n",
    "     #steps_per_epoch=(len(X)//4),callbacks=[checkpoint],\n",
    "    #epochs=30000)\n",
    "#model.fit(X, y, epochs=500, batch_size=32,callbacks=[es],validation_split=.1)\n",
    "\n",
    "test_loss, test_acc = model.evaluate(X, y)\n",
    "print('Test accuracy:', test_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the data augmentation\n",
    "train_datagen.fit(x_train)\n",
    "\n",
    "# setup generator\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(img_height, img_width),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical')\n",
    "\n",
    "# train model\n",
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=nb_train_samples,\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator, # optional - if used needs to be defined\n",
    "    validation_steps=nb_validation_samples) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('128_filters_100_color_2conv_max_batch2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_generator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-8b783e645587>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_indices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'train_generator' is not defined"
     ]
    }
   ],
   "source": [
    "train_generator.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=os.listdir(\"/Users/denisdunn/Desktop/aerin_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model=load_model('/Users/denisdunn/Downloads/feb_14th.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess('/Users/denisdunn/Desktop/aerin_test/Screen Shot 2020-02-11 at 12.30.00 PM.png', 28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path='/Users/denisdunn/Desktop/fourty_five_estee 2/S20_AERIN_WG_DIGITAL_SQUARE_02.jpg'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = image.load_img(img_path, target_size=(224, 224))\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = preprocess_input(x)\n",
    "\n",
    "features = model.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([38])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(features,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F19_AERIN_BCRF_SILO.tif\n"
     ]
    }
   ],
   "source": [
    "result=model.predict_classes(x)\n",
    "print(CATEGORIES[int(result)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE=15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_processing(test):\n",
    "    #array_test=img_to_array(test)\n",
    "    #img_array=cv2.imread(test)\n",
    "    array_resize=cv2.resize(test,(IMG_SIZE,IMG_SIZE))\n",
    "    test_image = array_resize[...,::-1]\n",
    "    return (np.array(test_image).reshape(1,IMG_SIZE,IMG_SIZE,3))/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_array = cv2.imread('/Users/denisdunn/Desktop/F19_AERIN_LIM_DIGITAL_1_R300.tif')#,cv2.IMREAD_GRAYSCALE)\n",
    "                \n",
    "new_array = cv2.resize(img_array,(IMG_SIZE,IMG_SIZE))\n",
    "test_image = new_array[...,::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "denis=cv2.imread('/Users/denisdunn/Desktop/191001_LSF_SHOT24_6612_2480DI_i_ADOBERGB.jpg',cv2.IMREAD_GRAYSCALE)\n",
    "#cv2.IMREAD_GRAYSCALE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_array = cv2.resize(img_array,(IMG_SIZE,IMG_SIZE))\n",
    "                test_image = new_array[...,::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer=image_processing(denis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([39])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_classes(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pic='/Users/denisdunn/Desktop/HOL19_AERIN_ MODEL_F72.tif'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pic_test(pic):\n",
    "    denis=cv2.imread(pic)#,cv2.IMREAD_GRAYSCALE)\n",
    "    answer=image_processing(denis)\n",
    "    result=model.predict_classes(answer)\n",
    "    direct='/Users/denisdunn/Desktop/full_aerin/{}/{}'.format(CATEGORIES[int(result)],CATEGORIES[int(result)])\n",
    "    return load_img(direct)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "direct='/Users/denisdunn/Desktop/full_aerin/{}/{}'.format(CATEGORIES[int(result)],CATEGORIES[int(result)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pic_test('/Users/denisdunn/Desktop/_0_117.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('feb_5_size_25_batch5_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.predict(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('/Users/denisdunn/Desktop/DATADIR/X',X)\n",
    "np.save('/Users/denisdunn/Desktop/DATADIR/y',y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "keras.layers.Conv2D(input_shape=(32,32,1),filters=64,kernel_size=(3,3),padding=\"same\", activation=\"relu\"),\n",
    "keras.layers.Conv2D(filters=64,kernel_size=(3,3),padding=\"same\", activation=\"relu\"),\n",
    "keras.layers.MaxPool2D(pool_size=(2,2),strides=(2,2)),\n",
    "keras.layers.Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"),\n",
    "keras.layers.Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"),\n",
    "keras.layers.MaxPool2D(pool_size=(2,2),strides=(2,2)),\n",
    "keras.layers.Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"),\n",
    "keras.layers.Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"),\n",
    "keras.layers.Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"),\n",
    "keras.layers.MaxPool2D(pool_size=(2,2),strides=(2,2)),\n",
    "keras.layers.Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"),\n",
    "keras.layers.Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"),\n",
    "keras.layers.Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"),\n",
    "keras.layers.MaxPool2D(pool_size=(2,2),strides=(2,2)),\n",
    "keras.layers.Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"),\n",
    "keras.layers.Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"),\n",
    "keras.layers.Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"),\n",
    "keras.layers.MaxPool2D(pool_size=(2,2),strides=(2,2)),\n",
    "keras.layers.Flatten(),\n",
    "keras.layers.Dense(units=4096,activation=\"relu\"),\n",
    "keras.layers.Dense(units=4096,activation=\"relu\"),\n",
    "keras.layers.Dense(units=44, activation=\"softmax\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "es=EarlyStopping(patience=20, monitor='acc',verbose=1)\n",
    "model.fit(X, y, epochs=1000, batch_size=10,callbacks=[es])\n",
    "\n",
    "test_loss, test_acc = model.evaluate(X, y)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the TensorBoard notebook extension.\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from datetime import datetime\n",
    "import io\n",
    "import itertools\n",
    "from packaging import version\n",
    "from six.moves import range\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn.metrics\n",
    "\n",
    "print(\"TensorFlow version: \", tf.__version__)\n",
    "assert version.parse(tf.__version__).release[0] >= 2, \\\n",
    "    \"This notebook requires TensorFlow 2.0 or above.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import cv2,time\n",
    "import numpy as np\n",
    "print(cv2.__version__)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "/anaconda3/lib/python3.7/site-packages/cv2/data/haarcascade_frontalface_default.xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_cascade=cv2.CascadeClassifier('/anaconda3/lib/python3.7/site-packages/cv2/data/haarcascade_frontalface_default.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_cascade=cv2.CascadeClassifier('file:///Users/denisdunn/opt/anaconda3/lib/python3.7/site-packages/cv2/data/haarcascade_frontalface_default.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_cascade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gray=cv2.imread('/Users/denisdunn/Desktop/Ivb19_0881-319x319.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faces=face_cascade.detectMultiScale(gray,scaleFactor=1.05,minNeighbors=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(faces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x,y,w,h, in faces:\n",
    "    img = cv2.rectangle(gray, (x,y), (x+w,y+h),(0,255,0),3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow('gray',img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import cv2\n",
    "\n",
    "#cvim2disp = cv2.imread('/Users/denisdunn/Desktop/Ivb19_0881-319x319.png')\n",
    "cv2.imshow('img', img)\n",
    "while(True):\n",
    "    k = cv2.waitKey(33)\n",
    "    if k == -1:  # if no key was pressed, -1 is returned\n",
    "        continue\n",
    "    else:\n",
    "        break\n",
    "cv2.destroyWindow('img')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video=cv2.VideoCapture(0)\n",
    "check,frame=video.read()\n",
    "time.sleep(3) \n",
    "cv2.imshow('Capture',frame)\n",
    "cv2.waitKey(0)\n",
    "video.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.destroyWindow('gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '/Users/denisdunn/Desktop/shirt_test.png'\n",
    "img = cv2.imread(filename)\n",
    "gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "gray = np.float32(gray)\n",
    "dst = cv2.cornerHarris(gray,2,3,0.04)\n",
    "\n",
    "#result is dilated for marking the corners, not important\n",
    "dst = cv2.dilate(dst,None)\n",
    "\n",
    "# Threshold for an optimal value, it may vary depending on the image.\n",
    "img[dst>0.01*dst.max()]=[0,0,255]\n",
    "\n",
    "cv2.imshow('dst',img)\n",
    "while(True):\n",
    "    k = cv2.waitKey(33)\n",
    "    if k == -1:  # if no key was pressed, -1 is returned\n",
    "        continue\n",
    "    else:\n",
    "        break\n",
    "cv2.destroyWindow('img')\n",
    "#if cv2.waitKey(0) & 0xff == 27:\n",
    "    #cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '/Users/denisdunn/Desktop/shirt_test.png'\n",
    "img = cv2.imread(filename)\n",
    "gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "gray = np.float32(gray)\n",
    "dst = cv2.cornerHarris(gray,2,3,0.04)\n",
    "\n",
    "#result is dilated for marking the corners, not important\n",
    "dst = cv2.dilate(dst,None)\n",
    "\n",
    "# Threshold for an optimal value, it may vary depending on the image.\n",
    "img[dst>0.01*dst.max()]=[0,0,255]\n",
    "\n",
    "cv2.imshow('dst',img)\n",
    "cv2.waitKey()\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(\"image.jpg\")\n",
    "cv2.startWindowThread()\n",
    "cv2.namedWindow(\"preview\")\n",
    "cv2.imshow(\"preview\", im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import save_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = load_img('/Users/denisdunn/Desktop/full_aerin/Aerin_Office_PhotoFrame_8x10.tif/Aerin_Office_PhotoFrame_8x10.tif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_array = img_to_array(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_img('office_photoframe.jpg', img_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
